{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UGATIT_predict.ipynb","provenance":[],"collapsed_sections":["W14SklNDQRp3","6pZCuvQFDVjo"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"159n4hXx5evgBVSA2WzK3GR6sY9xxgbHX","authorship_tag":"ABX9TyM/g13S9Rvv/tWL02SsQCNp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWLRypLUdymp","executionInfo":{"status":"ok","timestamp":1610097854629,"user_tz":-60,"elapsed":2744,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"2f3153ba-cbb9-457e-a0b9-cfe31c15fc8e"},"source":["!nvidia-smi -L\n","!pip install --upgrade --force-reinstall --no-deps kaggle"],"execution_count":1,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-0764ff1a-9ab3-d5fc-d62a-14ce0e5b2c92)\n","Collecting kaggle\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/33/365c0d13f07a2a54744d027fe20b60dacdfdfb33bc04746db6ad0b79340b/kaggle-1.5.10.tar.gz (59kB)\n","\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.10-cp36-none-any.whl size=73269 sha256=fcd223771e7ad3cbb86c1c38a339fef7b998dc251a935502bb535d847bfa0855\n","  Stored in directory: /root/.cache/pip/wheels/3a/d1/7e/6ce09b72b770149802c653a02783821629146983ee5a360f10\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Found existing installation: kaggle 1.5.10\n","    Uninstalling kaggle-1.5.10:\n","      Successfully uninstalled kaggle-1.5.10\n","Successfully installed kaggle-1.5.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RVBG-hkSD4po","executionInfo":{"status":"ok","timestamp":1610097932081,"user_tz":-60,"elapsed":3216,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.nn.parameter import Parameter\n","import torch.utils.data as data\n","\n","import numpy as np\n","import cv2\n","from scipy import misc\n","import time, itertools\n","\n","from PIL import Image\n","\n","import os\n","import os.path"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5jk3g9Jd54L"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"zbg0j_NSd7Uh"},"source":["from google.colab import files\n","files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AxCtfHOneAH2"},"source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","! kaggle datasets list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gt93ENw1eAxB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610097964816,"user_tz":-60,"elapsed":5945,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"2229b0ea-e226-48ec-94a7-a422bef507cd"},"source":["! kaggle competitions download -c gan-getting-started"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading gan-getting-started.zip to /content\n"," 97% 356M/367M [00:04<00:00, 127MB/s]\n","100% 367M/367M [00:04<00:00, 81.3MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C_L190GHeE16"},"source":["! unzip /content/gan-getting-started.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W14SklNDQRp3"},"source":["# Utils"]},{"cell_type":"code","metadata":{"id":"V3LhKG0OQTDA","executionInfo":{"status":"ok","timestamp":1610097992233,"user_tz":-60,"elapsed":576,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["def load_test_data(image_path, size=256):\n","    img = misc.imread(image_path, mode='RGB')\n","    img = misc.imresize(img, [size, size])\n","    img = np.expand_dims(img, axis=0)\n","    img = preprocessing(img)\n","\n","    return img\n","\n","def preprocessing(x):\n","    x = x/127.5 - 1 # -1 ~ 1\n","    return x\n","\n","def save_images(images, size, image_path):\n","    return imsave(inverse_transform(images), size, image_path)\n","\n","def inverse_transform(images):\n","    return (images+1.) / 2\n","\n","def imsave(images, size, path):\n","    return misc.imsave(path, merge(images, size))\n","\n","def merge(images, size):\n","    h, w = images.shape[1], images.shape[2]\n","    img = np.zeros((h * size[0], w * size[1], 3))\n","    for idx, image in enumerate(images):\n","        i = idx % size[1]\n","        j = idx // size[1]\n","        img[h*j:h*(j+1), w*i:w*(i+1), :] = image\n","\n","    return img\n","\n","def check_folder(log_dir):\n","    if not os.path.exists(log_dir):\n","        os.makedirs(log_dir)\n","    return log_dir\n","\n","def str2bool(x):\n","    return x.lower() in ('true')\n","\n","def cam(x, size = 256):\n","    x = x - np.min(x)\n","    cam_img = x / np.max(x)\n","    cam_img = np.uint8(255 * cam_img)\n","    cam_img = cv2.resize(cam_img, (size, size))\n","    cam_img = cv2.applyColorMap(cam_img, cv2.COLORMAP_JET)\n","    return cam_img / 255.0\n","\n","def imagenet_norm(x):\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.299, 0.224, 0.225]\n","    mean = torch.FloatTensor(mean).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(x.device)\n","    std = torch.FloatTensor(std).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(x.device)\n","    return (x - mean) / std\n","\n","def denorm(x):\n","    return x * 0.5 + 0.5\n","\n","def tensor2numpy(x):\n","    return x.detach().cpu().numpy().transpose(1,2,0)\n","\n","def RGB2BGR(x):\n","    return cv2.cvtColor(x, cv2.COLOR_RGB2BGR)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pZCuvQFDVjo"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"Mi7ujCXWCE3n","executionInfo":{"status":"ok","timestamp":1610097995326,"user_tz":-60,"elapsed":1061,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["class ResnetGenerator(nn.Module):\n","    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=6, img_size=256, light=False):\n","        assert(n_blocks >= 0)\n","        super(ResnetGenerator, self).__init__()\n","        self.input_nc = input_nc\n","        self.output_nc = output_nc\n","        self.ngf = ngf\n","        self.n_blocks = n_blocks\n","        self.img_size = img_size\n","        self.light = light\n","\n","        DownBlock = []\n","        DownBlock += [nn.ReflectionPad2d(3),\n","                      nn.Conv2d(input_nc, ngf, kernel_size=7, stride=1, padding=0, bias=False),\n","                      nn.InstanceNorm2d(ngf),\n","                      nn.ReLU(True)]\n","\n","        # Down-Sampling\n","        n_downsampling = 2\n","        for i in range(n_downsampling):\n","            mult = 2**i\n","            DownBlock += [nn.ReflectionPad2d(1),\n","                          nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=0, bias=False),\n","                          nn.InstanceNorm2d(ngf * mult * 2),\n","                          nn.ReLU(True)]\n","\n","        # Down-Sampling Bottleneck\n","        mult = 2**n_downsampling\n","        for i in range(n_blocks):\n","            DownBlock += [ResnetBlock(ngf * mult, use_bias=False)]\n","\n","        # Class Activation Map\n","        self.gap_fc = nn.Linear(ngf * mult, 1, bias=False)\n","        self.gmp_fc = nn.Linear(ngf * mult, 1, bias=False)\n","        self.conv1x1 = nn.Conv2d(ngf * mult * 2, ngf * mult, kernel_size=1, stride=1, bias=True)\n","        self.relu = nn.ReLU(True)\n","\n","        # Gamma, Beta block\n","        if self.light:\n","            FC = [nn.Linear(ngf * mult, ngf * mult, bias=False),\n","                  nn.ReLU(True),\n","                  nn.Linear(ngf * mult, ngf * mult, bias=False),\n","                  nn.ReLU(True)]\n","        else:\n","            FC = [nn.Linear(img_size // mult * img_size // mult * ngf * mult, ngf * mult, bias=False),\n","                  nn.ReLU(True),\n","                  nn.Linear(ngf * mult, ngf * mult, bias=False),\n","                  nn.ReLU(True)]\n","        self.gamma = nn.Linear(ngf * mult, ngf * mult, bias=False)\n","        self.beta = nn.Linear(ngf * mult, ngf * mult, bias=False)\n","\n","        # Up-Sampling Bottleneck\n","        for i in range(n_blocks):\n","            setattr(self, 'UpBlock1_' + str(i+1), ResnetAdaILNBlock(ngf * mult, use_bias=False))\n","\n","        # Up-Sampling\n","        UpBlock2 = []\n","        for i in range(n_downsampling):\n","            mult = 2**(n_downsampling - i)\n","            UpBlock2 += [nn.Upsample(scale_factor=2, mode='nearest'),\n","                         nn.ReflectionPad2d(1),\n","                         nn.Conv2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=1, padding=0, bias=False),\n","                         ILN(int(ngf * mult / 2)),\n","                         nn.ReLU(True)]\n","\n","        UpBlock2 += [nn.ReflectionPad2d(3),\n","                     nn.Conv2d(ngf, output_nc, kernel_size=7, stride=1, padding=0, bias=False),\n","                     nn.Tanh()]\n","\n","        self.DownBlock = nn.Sequential(*DownBlock)\n","        self.FC = nn.Sequential(*FC)\n","        self.UpBlock2 = nn.Sequential(*UpBlock2)\n","\n","    def forward(self, input):\n","        x = self.DownBlock(input)\n","\n","        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n","        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))\n","        gap_weight = list(self.gap_fc.parameters())[0]\n","        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)\n","\n","        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)\n","        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))\n","        gmp_weight = list(self.gmp_fc.parameters())[0]\n","        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n","\n","        cam_logit = torch.cat([gap_logit, gmp_logit], 1)\n","        x = torch.cat([gap, gmp], 1)\n","        x = self.relu(self.conv1x1(x))\n","\n","        heatmap = torch.sum(x, dim=1, keepdim=True)\n","\n","        if self.light:\n","            x_ = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n","            x_ = self.FC(x_.view(x_.shape[0], -1))\n","        else:\n","            x_ = self.FC(x.view(x.shape[0], -1))\n","        gamma, beta = self.gamma(x_), self.beta(x_)\n","\n","\n","        for i in range(self.n_blocks):\n","            x = getattr(self, 'UpBlock1_' + str(i+1))(x, gamma, beta)\n","        out = self.UpBlock2(x)\n","\n","        return out, cam_logit, heatmap\n","\n","\n","class ResnetBlock(nn.Module):\n","    def __init__(self, dim, use_bias):\n","        super(ResnetBlock, self).__init__()\n","        conv_block = []\n","        conv_block += [nn.ReflectionPad2d(1),\n","                       nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias),\n","                       nn.InstanceNorm2d(dim),\n","                       nn.ReLU(True)]\n","\n","        conv_block += [nn.ReflectionPad2d(1),\n","                       nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias),\n","                       nn.InstanceNorm2d(dim)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        out = x + self.conv_block(x)\n","        return out\n","\n","\n","class ResnetAdaILNBlock(nn.Module):\n","    def __init__(self, dim, use_bias):\n","        super(ResnetAdaILNBlock, self).__init__()\n","        self.pad1 = nn.ReflectionPad2d(1)\n","        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias)\n","        self.norm1 = adaILN(dim)\n","        self.relu1 = nn.ReLU(True)\n","\n","        self.pad2 = nn.ReflectionPad2d(1)\n","        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias)\n","        self.norm2 = adaILN(dim)\n","\n","    def forward(self, x, gamma, beta):\n","        out = self.pad1(x)\n","        out = self.conv1(out)\n","        out = self.norm1(out, gamma, beta)\n","        out = self.relu1(out)\n","        out = self.pad2(out)\n","        out = self.conv2(out)\n","        out = self.norm2(out, gamma, beta)\n","\n","        return out + x\n","\n","\n","class adaILN(nn.Module):\n","    def __init__(self, num_features, eps=1e-5):\n","        super(adaILN, self).__init__()\n","        self.eps = eps\n","        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n","        self.rho.data.fill_(0.9)\n","\n","    def forward(self, input, gamma, beta):\n","        in_mean, in_var = torch.mean(input, dim=[2, 3], keepdim=True), torch.var(input, dim=[2, 3], keepdim=True)\n","        out_in = (input - in_mean) / torch.sqrt(in_var + self.eps)\n","        ln_mean, ln_var = torch.mean(input, dim=[1, 2, 3], keepdim=True), torch.var(input, dim=[1, 2, 3], keepdim=True)\n","        out_ln = (input - ln_mean) / torch.sqrt(ln_var + self.eps)\n","        out = self.rho.expand(input.shape[0], -1, -1, -1) * out_in + (1-self.rho.expand(input.shape[0], -1, -1, -1)) * out_ln\n","        out = out * gamma.unsqueeze(2).unsqueeze(3) + beta.unsqueeze(2).unsqueeze(3)\n","\n","        return out\n","\n","\n","class ILN(nn.Module):\n","    def __init__(self, num_features, eps=1e-5):\n","        super(ILN, self).__init__()\n","        self.eps = eps\n","        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n","        self.gamma = Parameter(torch.Tensor(1, num_features, 1, 1))\n","        self.beta = Parameter(torch.Tensor(1, num_features, 1, 1))\n","        self.rho.data.fill_(0.0)\n","        self.gamma.data.fill_(1.0)\n","        self.beta.data.fill_(0.0)\n","\n","    def forward(self, input):\n","        in_mean, in_var = torch.mean(input, dim=[2, 3], keepdim=True), torch.var(input, dim=[2, 3], keepdim=True)\n","        out_in = (input - in_mean) / torch.sqrt(in_var + self.eps)\n","        ln_mean, ln_var = torch.mean(input, dim=[1, 2, 3], keepdim=True), torch.var(input, dim=[1, 2, 3], keepdim=True)\n","        out_ln = (input - ln_mean) / torch.sqrt(ln_var + self.eps)\n","        out = self.rho.expand(input.shape[0], -1, -1, -1) * out_in + (1-self.rho.expand(input.shape[0], -1, -1, -1)) * out_ln\n","        out = out * self.gamma.expand(input.shape[0], -1, -1, -1) + self.beta.expand(input.shape[0], -1, -1, -1)\n","\n","        return out\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=5):\n","        super(Discriminator, self).__init__()\n","        model = [nn.ReflectionPad2d(1),\n","                 nn.utils.spectral_norm(\n","                 nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=0, bias=True)),\n","                 nn.LeakyReLU(0.2, True)]\n","\n","        for i in range(1, n_layers - 2):\n","            mult = 2 ** (i - 1)\n","            model += [nn.ReflectionPad2d(1),\n","                      nn.utils.spectral_norm(\n","                      nn.Conv2d(ndf * mult, ndf * mult * 2, kernel_size=4, stride=2, padding=0, bias=True)),\n","                      nn.LeakyReLU(0.2, True)]\n","\n","        mult = 2 ** (n_layers - 2 - 1)\n","        model += [nn.ReflectionPad2d(1),\n","                  nn.utils.spectral_norm(\n","                  nn.Conv2d(ndf * mult, ndf * mult * 2, kernel_size=4, stride=1, padding=0, bias=True)),\n","                  nn.LeakyReLU(0.2, True)]\n","\n","        # Class Activation Map\n","        mult = 2 ** (n_layers - 2)\n","        self.gap_fc = nn.utils.spectral_norm(nn.Linear(ndf * mult, 1, bias=False))\n","        self.gmp_fc = nn.utils.spectral_norm(nn.Linear(ndf * mult, 1, bias=False))\n","        self.conv1x1 = nn.Conv2d(ndf * mult * 2, ndf * mult, kernel_size=1, stride=1, bias=True)\n","        self.leaky_relu = nn.LeakyReLU(0.2, True)\n","\n","        self.pad = nn.ReflectionPad2d(1)\n","        self.conv = nn.utils.spectral_norm(\n","            nn.Conv2d(ndf * mult, 1, kernel_size=4, stride=1, padding=0, bias=False))\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input):\n","        x = self.model(input)\n","\n","        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n","        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))\n","        gap_weight = list(self.gap_fc.parameters())[0]\n","        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)\n","\n","        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)\n","        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))\n","        gmp_weight = list(self.gmp_fc.parameters())[0]\n","        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n","\n","        cam_logit = torch.cat([gap_logit, gmp_logit], 1)\n","        x = torch.cat([gap, gmp], 1)\n","        x = self.leaky_relu(self.conv1x1(x))\n","\n","        heatmap = torch.sum(x, dim=1, keepdim=True)\n","\n","        x = self.pad(x)\n","        out = self.conv(x)\n","\n","        return out, cam_logit, heatmap\n","\n","\n","class RhoClipper(object):\n","\n","    def __init__(self, min, max):\n","        self.clip_min = min\n","        self.clip_max = max\n","        assert min < max\n","\n","    def __call__(self, module):\n","\n","        if hasattr(module, 'rho'):\n","            w = module.rho.data\n","            w = w.clamp(self.clip_min, self.clip_max)\n","            module.rho.data = w"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7KroGBaK9r5"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"CpA-deDyK8qR","executionInfo":{"status":"ok","timestamp":1610097998908,"user_tz":-60,"elapsed":547,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["def has_file_allowed_extension(filename, extensions):\n","    \"\"\"Checks if a file is an allowed extension.\n","    Args:\n","        filename (string): path to a file\n","    Returns:\n","        bool: True if the filename ends with a known image extension\n","    \"\"\"\n","    filename_lower = filename.lower()\n","    return any(filename_lower.endswith(ext) for ext in extensions)\n","\n","\n","def find_classes(dir):\n","    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n","    classes.sort()\n","    class_to_idx = {classes[i]: i for i in range(len(classes))}\n","    return classes, class_to_idx\n","\n","\n","def make_dataset(dir, extensions):\n","    images = []\n","    for root, _, fnames in sorted(os.walk(dir)):\n","        for fname in sorted(fnames):\n","            if has_file_allowed_extension(fname, extensions):\n","                path = os.path.join(root, fname)\n","                item = (path, 0)\n","                images.append(item)\n","\n","    return images\n","\n","\n","class DatasetFolder(data.Dataset):\n","    def __init__(self, root, loader, extensions, transform=None, target_transform=None):\n","        # classes, class_to_idx = find_classes(root)\n","        samples = make_dataset(root, extensions)\n","        if len(samples) == 0:\n","            raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n","                               \"Supported extensions are: \" + \",\".join(extensions)))\n","\n","        self.root = root\n","        self.loader = loader\n","        self.extensions = extensions\n","        self.samples = samples\n","\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","        Returns:\n","            tuple: (sample, target) where target is class_index of the target class.\n","        \"\"\"\n","        path, target = self.samples[index]\n","        sample = self.loader(path)\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return sample, target\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __repr__(self):\n","        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n","        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n","        fmt_str += '    Root Location: {}\\n'.format(self.root)\n","        tmp = '    Transforms (if any): '\n","        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n","        tmp = '    Target Transforms (if any): '\n","        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n","        return fmt_str\n","\n","\n","IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n","\n","\n","def pil_loader(path):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert('RGB')\n","\n","\n","def default_loader(path):\n","    return pil_loader(path)\n","\n","\n","class ImageFolder(DatasetFolder):\n","    def __init__(self, root, transform=None, target_transform=None,\n","                 loader=default_loader):\n","        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n","                                          transform=transform,\n","                                          target_transform=target_transform)\n","        self.imgs = self.samples"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"aC32CNeTDUjy","executionInfo":{"status":"ok","timestamp":1610098002766,"user_tz":-60,"elapsed":1385,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["class UGATIT(object) :\n","    def __init__(self, trainA_path=None, trainB_path=None, resume=False, model_path='/content/drive/MyDrive/photo2monet/models/', \n","                 name_model=None, output_path='/content/drive/MyDrive/photo2monet/outputs/', submissions_path=None):  ## A2B\n","        self.light = True\n","        self.model_name = 'UGATIT'\n","\n","        #self.result_dir = args.result_dir\n","        self.trainApath = trainA_path\n","        self.trainBpath = trainB_path\n","\n","        self.iteration = 100000\n","        self.decay_flag = True\n","\n","        self.batch_size = 1\n","\n","        self.lr = 0.0001\n","        self.weight_decay = 0.0001\n","        self.ch = 64\n","\n","        \"\"\" Weight \"\"\"\n","        self.adv_weight = 1\n","        self.cycle_weight = 10\n","        self.identity_weight = 10\n","        self.cam_weight = 1000\n","\n","        \"\"\" Generator \"\"\"\n","        self.n_res = 4\n","\n","        \"\"\" Discriminator \"\"\"\n","        self.n_dis = 6\n","\n","        self.img_size = 256\n","        self.img_ch = 3\n","\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.resume = resume\n","        self.model_path = model_path\n","        self.name_model = name_model\n","        self.output_path = output_path\n","        self.submissions_path = submissions_path\n","        self.print_freq = 400\n","\n","\n","    ##################################################################################\n","    # Model\n","    ##################################################################################\n","\n","    def build_model(self):\n","        \"\"\" DataLoader \"\"\"\n","        train_transform = transforms.Compose([\n","            transforms.RandomHorizontalFlip(p=0.4),\n","            transforms.RandomVerticalFlip(p=0.4),\n","            #transforms.Resize((self.img_size, self.img_size)),\n","            transforms.RandomResizedCrop(self.img_size, scale=(0.6,1.0)),\n","            #transforms.RandomRotation(15),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                                 std=[0.5, 0.5, 0.5])\n","        ])\n","        test_transform = transforms.Compose([\n","            transforms.Resize((self.img_size, self.img_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                                 std=[0.5, 0.5, 0.5])\n","        ])\n","\n","        self.trainA = ImageFolder(self.trainApath, train_transform)\n","        self.trainB = ImageFolder(self.trainBpath, train_transform)\n","        self.testA = ImageFolder(self.trainApath, test_transform)\n","        self.testB = ImageFolder(self.trainBpath, test_transform)\n","        self.trainA_loader = DataLoader(self.trainA, batch_size=self.batch_size, shuffle=True)\n","        self.trainB_loader = DataLoader(self.trainB, batch_size=self.batch_size, shuffle=True)\n","        self.testA_loader = DataLoader(self.testA, batch_size=1, shuffle=False)\n","        self.testB_loader = DataLoader(self.testB, batch_size=1, shuffle=False)\n","\n","        \"\"\" Define Generator, Discriminator \"\"\"\n","        self.genA2B = ResnetGenerator(input_nc=3, output_nc=3, ngf=self.ch, n_blocks=self.n_res, img_size=self.img_size, light=self.light).to(self.device)\n","        self.genB2A = ResnetGenerator(input_nc=3, output_nc=3, ngf=self.ch, n_blocks=self.n_res, img_size=self.img_size, light=self.light).to(self.device)\n","        self.disGA = Discriminator(input_nc=3, ndf=self.ch, n_layers=7).to(self.device)\n","        self.disGB = Discriminator(input_nc=3, ndf=self.ch, n_layers=7).to(self.device)\n","        self.disLA = Discriminator(input_nc=3, ndf=self.ch, n_layers=5).to(self.device)\n","        self.disLB = Discriminator(input_nc=3, ndf=self.ch, n_layers=5).to(self.device)\n","\n","        \"\"\" Define Loss \"\"\"\n","        self.L1_loss = nn.L1Loss().to(self.device)\n","        self.MSE_loss = nn.MSELoss().to(self.device)\n","        self.BCE_loss = nn.BCEWithLogitsLoss().to(self.device)\n","\n","        \"\"\" Trainer \"\"\"\n","        self.G_optim = torch.optim.Adam(itertools.chain(self.genA2B.parameters(), self.genB2A.parameters()), lr=self.lr, betas=(0.5, 0.999), weight_decay=self.weight_decay)\n","        self.D_optim = torch.optim.Adam(itertools.chain(self.disGA.parameters(), self.disGB.parameters(), self.disLA.parameters(), self.disLB.parameters()), lr=self.lr, betas=(0.5, 0.999), weight_decay=self.weight_decay)\n","\n","        \"\"\" Define Rho clipper to constraint the value of rho in AdaILN and ILN\"\"\"\n","        self.Rho_clipper = RhoClipper(0, 1)\n","\n","    def train(self):\n","        self.genA2B.train(), self.genB2A.train(), self.disGA.train(), self.disGB.train(), self.disLA.train(), self.disLB.train()\n","\n","        start_iter = 1\n","        if self.resume:\n","            self.load(self.model_path + self.name_model)\n","            print(\" [*] Load SUCCESS\")\n","            if self.decay_flag and start_iter > (self.iteration // 2):\n","                self.G_optim.param_groups[0]['lr'] -= (self.lr / (self.iteration // 2)) * (start_iter - self.iteration // 2)\n","                self.D_optim.param_groups[0]['lr'] -= (self.lr / (self.iteration // 2)) * (start_iter - self.iteration // 2)\n","\n","        # training loop\n","        print('training start !')\n","        start_time = time.time()\n","        for step in range(start_iter, self.iteration + 1):\n","            if self.decay_flag and step > (self.iteration // 2):\n","                self.G_optim.param_groups[0]['lr'] -= (self.lr / (self.iteration // 2))\n","                self.D_optim.param_groups[0]['lr'] -= (self.lr / (self.iteration // 2))\n","\n","            try:\n","                real_A, _ = trainA_iter.next()\n","            except:\n","                trainA_iter = iter(self.trainA_loader)\n","                real_A, _ = trainA_iter.next()\n","\n","            try:\n","                real_B, _ = trainB_iter.next()\n","            except:\n","                trainB_iter = iter(self.trainB_loader)\n","                real_B, _ = trainB_iter.next()\n","\n","            real_A, real_B = real_A.to(self.device), real_B.to(self.device)\n","\n","            # Update D\n","            self.D_optim.zero_grad()\n","\n","            fake_A2B, _, _ = self.genA2B(real_A)\n","            fake_B2A, _, _ = self.genB2A(real_B)\n","\n","            real_GA_logit, real_GA_cam_logit, _ = self.disGA(real_A)\n","            real_LA_logit, real_LA_cam_logit, _ = self.disLA(real_A)\n","            real_GB_logit, real_GB_cam_logit, _ = self.disGB(real_B)\n","            real_LB_logit, real_LB_cam_logit, _ = self.disLB(real_B)\n","\n","            fake_GA_logit, fake_GA_cam_logit, _ = self.disGA(fake_B2A)\n","            fake_LA_logit, fake_LA_cam_logit, _ = self.disLA(fake_B2A)\n","            fake_GB_logit, fake_GB_cam_logit, _ = self.disGB(fake_A2B)\n","            fake_LB_logit, fake_LB_cam_logit, _ = self.disLB(fake_A2B)\n","\n","            D_ad_loss_GA = self.MSE_loss(real_GA_logit, torch.ones_like(real_GA_logit).to(self.device)) + self.MSE_loss(fake_GA_logit, torch.zeros_like(fake_GA_logit).to(self.device))\n","            D_ad_cam_loss_GA = self.MSE_loss(real_GA_cam_logit, torch.ones_like(real_GA_cam_logit).to(self.device)) + self.MSE_loss(fake_GA_cam_logit, torch.zeros_like(fake_GA_cam_logit).to(self.device))\n","            D_ad_loss_LA = self.MSE_loss(real_LA_logit, torch.ones_like(real_LA_logit).to(self.device)) + self.MSE_loss(fake_LA_logit, torch.zeros_like(fake_LA_logit).to(self.device))\n","            D_ad_cam_loss_LA = self.MSE_loss(real_LA_cam_logit, torch.ones_like(real_LA_cam_logit).to(self.device)) + self.MSE_loss(fake_LA_cam_logit, torch.zeros_like(fake_LA_cam_logit).to(self.device))\n","            D_ad_loss_GB = self.MSE_loss(real_GB_logit, torch.ones_like(real_GB_logit).to(self.device)) + self.MSE_loss(fake_GB_logit, torch.zeros_like(fake_GB_logit).to(self.device))\n","            D_ad_cam_loss_GB = self.MSE_loss(real_GB_cam_logit, torch.ones_like(real_GB_cam_logit).to(self.device)) + self.MSE_loss(fake_GB_cam_logit, torch.zeros_like(fake_GB_cam_logit).to(self.device))\n","            D_ad_loss_LB = self.MSE_loss(real_LB_logit, torch.ones_like(real_LB_logit).to(self.device)) + self.MSE_loss(fake_LB_logit, torch.zeros_like(fake_LB_logit).to(self.device))\n","            D_ad_cam_loss_LB = self.MSE_loss(real_LB_cam_logit, torch.ones_like(real_LB_cam_logit).to(self.device)) + self.MSE_loss(fake_LB_cam_logit, torch.zeros_like(fake_LB_cam_logit).to(self.device))\n","\n","            D_loss_A = self.adv_weight * (D_ad_loss_GA + D_ad_cam_loss_GA + D_ad_loss_LA + D_ad_cam_loss_LA)\n","            D_loss_B = self.adv_weight * (D_ad_loss_GB + D_ad_cam_loss_GB + D_ad_loss_LB + D_ad_cam_loss_LB)\n","\n","            Discriminator_loss = D_loss_A + D_loss_B\n","            Discriminator_loss.backward()\n","            self.D_optim.step()\n","\n","            # Update G\n","            self.G_optim.zero_grad()\n","\n","            fake_A2B, fake_A2B_cam_logit, _ = self.genA2B(real_A)\n","            fake_B2A, fake_B2A_cam_logit, _ = self.genB2A(real_B)\n","\n","            fake_A2B2A, _, _ = self.genB2A(fake_A2B)\n","            fake_B2A2B, _, _ = self.genA2B(fake_B2A)\n","\n","            fake_A2A, fake_A2A_cam_logit, _ = self.genB2A(real_A)\n","            fake_B2B, fake_B2B_cam_logit, _ = self.genA2B(real_B)\n","\n","            fake_GA_logit, fake_GA_cam_logit, _ = self.disGA(fake_B2A)\n","            fake_LA_logit, fake_LA_cam_logit, _ = self.disLA(fake_B2A)\n","            fake_GB_logit, fake_GB_cam_logit, _ = self.disGB(fake_A2B)\n","            fake_LB_logit, fake_LB_cam_logit, _ = self.disLB(fake_A2B)\n","\n","            G_ad_loss_GA = self.MSE_loss(fake_GA_logit, torch.ones_like(fake_GA_logit).to(self.device))\n","            G_ad_cam_loss_GA = self.MSE_loss(fake_GA_cam_logit, torch.ones_like(fake_GA_cam_logit).to(self.device))\n","            G_ad_loss_LA = self.MSE_loss(fake_LA_logit, torch.ones_like(fake_LA_logit).to(self.device))\n","            G_ad_cam_loss_LA = self.MSE_loss(fake_LA_cam_logit, torch.ones_like(fake_LA_cam_logit).to(self.device))\n","            G_ad_loss_GB = self.MSE_loss(fake_GB_logit, torch.ones_like(fake_GB_logit).to(self.device))\n","            G_ad_cam_loss_GB = self.MSE_loss(fake_GB_cam_logit, torch.ones_like(fake_GB_cam_logit).to(self.device))\n","            G_ad_loss_LB = self.MSE_loss(fake_LB_logit, torch.ones_like(fake_LB_logit).to(self.device))\n","            G_ad_cam_loss_LB = self.MSE_loss(fake_LB_cam_logit, torch.ones_like(fake_LB_cam_logit).to(self.device))\n","\n","            G_recon_loss_A = self.L1_loss(fake_A2B2A, real_A)\n","            G_recon_loss_B = self.L1_loss(fake_B2A2B, real_B)\n","\n","            G_identity_loss_A = self.L1_loss(fake_A2A, real_A)\n","            G_identity_loss_B = self.L1_loss(fake_B2B, real_B)\n","\n","            G_cam_loss_A = self.BCE_loss(fake_B2A_cam_logit, torch.ones_like(fake_B2A_cam_logit).to(self.device)) + self.BCE_loss(fake_A2A_cam_logit, torch.zeros_like(fake_A2A_cam_logit).to(self.device))\n","            G_cam_loss_B = self.BCE_loss(fake_A2B_cam_logit, torch.ones_like(fake_A2B_cam_logit).to(self.device)) + self.BCE_loss(fake_B2B_cam_logit, torch.zeros_like(fake_B2B_cam_logit).to(self.device))\n","\n","            G_loss_A =  self.adv_weight * (G_ad_loss_GA + G_ad_cam_loss_GA + G_ad_loss_LA + G_ad_cam_loss_LA) + self.cycle_weight * G_recon_loss_A + self.identity_weight * G_identity_loss_A + self.cam_weight * G_cam_loss_A\n","            G_loss_B = self.adv_weight * (G_ad_loss_GB + G_ad_cam_loss_GB + G_ad_loss_LB + G_ad_cam_loss_LB) + self.cycle_weight * G_recon_loss_B + self.identity_weight * G_identity_loss_B + self.cam_weight * G_cam_loss_B\n","            \n","            del real_A, real_B, fake_A2B, fake_B2A, fake_A2B2A, fake_B2A2B, fake_A2A, fake_B2B \n","            torch.cuda.empty_cache()\n","            Generator_loss = G_loss_A + G_loss_B\n","            Generator_loss.backward()\n","            self.G_optim.step()\n","\n","            # clip parameter of AdaILN and ILN, applied after optimizer step\n","            self.genA2B.apply(self.Rho_clipper)\n","            self.genB2A.apply(self.Rho_clipper)\n","            if step % 100 == 0:\n","              print(\"[%5d/%5d] time: %4.4f d_loss: %.8f, g_loss: %.8f\" % (step, self.iteration, time.time() - start_time, Discriminator_loss, Generator_loss))\n","            if step % self.print_freq == 0:\n","                test_sample_num = 4\n","                A2B = np.zeros((self.img_size * 7, 0, 3))\n","                B2A = np.zeros((self.img_size * 7, 0, 3))\n","\n","                self.genA2B.eval(), self.genB2A.eval(), self.disGA.eval(), self.disGB.eval(), self.disLA.eval(), self.disLB.eval()\n","                for _ in range(test_sample_num):\n","                    try:\n","                        real_A, _ = testA_iter.next()\n","                    except:\n","                        testA_iter = iter(self.testA_loader)\n","                        real_A, _ = testA_iter.next()\n","\n","                    try:\n","                        real_B, _ = testB_iter.next()\n","                    except:\n","                        testB_iter = iter(self.testB_loader)\n","                        real_B, _ = testB_iter.next()\n","                    real_A, real_B = real_A.to(self.device), real_B.to(self.device)\n","\n","                    fake_A2B, _, fake_A2B_heatmap = self.genA2B(real_A)\n","                    fake_B2A, _, fake_B2A_heatmap = self.genB2A(real_B)\n","\n","                    fake_A2B2A, _, fake_A2B2A_heatmap = self.genB2A(fake_A2B)\n","                    fake_B2A2B, _, fake_B2A2B_heatmap = self.genA2B(fake_B2A)\n","\n","                    fake_A2A, _, fake_A2A_heatmap = self.genB2A(real_A)\n","                    fake_B2B, _, fake_B2B_heatmap = self.genA2B(real_B)\n","\n","                    A2B = np.concatenate((A2B, np.concatenate((RGB2BGR(tensor2numpy(denorm(real_A[0]))),\n","                                                               cam(tensor2numpy(fake_A2A_heatmap[0]), self.img_size),\n","                                                               RGB2BGR(tensor2numpy(denorm(fake_A2A[0]))),\n","                                                               cam(tensor2numpy(fake_A2B_heatmap[0]), self.img_size),\n","                                                               RGB2BGR(tensor2numpy(denorm(fake_A2B[0]))),\n","                                                               cam(tensor2numpy(fake_A2B2A_heatmap[0]), self.img_size),\n","                                                               RGB2BGR(tensor2numpy(denorm(fake_A2B2A[0])))), 0)), 1)\n","\n","                cv2.imwrite(self.output_path + 'A2B_%d.png'%step, A2B * 255.0)\n","                self.genA2B.train(), self.genB2A.train(), self.disGA.train(), self.disGB.train(), self.disLA.train(), self.disLB.train()\n","\n","            if step % 2000 == 0:\n","                params = {}\n","                params['genA2B'] = self.genA2B.state_dict()\n","                params['genB2A'] = self.genB2A.state_dict()\n","                params['disGA'] = self.disGA.state_dict()\n","                params['disGB'] = self.disGB.state_dict()\n","                params['disLA'] = self.disLA.state_dict()\n","                params['disLB'] = self.disLB.state_dict()\n","                torch.save(params, self.model_path + 'params_latest.pt')\n","        self.save()\n","\n","    def save(self):\n","        params = {}\n","        params['genA2B'] = self.genA2B.state_dict()\n","        params['genB2A'] = self.genB2A.state_dict()\n","        params['disGA'] = self.disGA.state_dict()\n","        params['disGB'] = self.disGB.state_dict()\n","        params['disLA'] = self.disLA.state_dict()\n","        params['disLB'] = self.disLB.state_dict()\n","        torch.save(params, self.model_path + 'params_latest.pt')\n","\n","    def load(self, path):\n","        params = torch.load(path)\n","        self.genA2B.load_state_dict(params['genA2B'])\n","        self.genB2A.load_state_dict(params['genB2A'])\n","        self.disGA.load_state_dict(params['disGA'])\n","        self.disGB.load_state_dict(params['disGB'])\n","        self.disLA.load_state_dict(params['disLA'])\n","        self.disLB.load_state_dict(params['disLB'])\n","\n","    def test(self):\n","        self.load(self.model_path + self.name_model)\n","        print(\" [*] Load SUCCESS\")\n","\n","        self.genA2B.eval(), self.genB2A.eval()\n","        for n, (real_A, _) in enumerate(self.testA_loader):\n","            if n % 200 == 0:\n","              print(f'{n} Monet Picture have been generated')\n","            real_A = real_A.to(self.device)\n","\n","            fake_A2B, _, fake_A2B_heatmap = self.genA2B(real_A)\n","\n","            A2B = RGB2BGR(tensor2numpy(denorm(fake_A2B[0])))\n","\n","            cv2.imwrite(self.submissions_path + '%d.jpg'%(n + 1), A2B * 255.0)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"elIYdh-nfCyK","executionInfo":{"status":"ok","timestamp":1610098040235,"user_tz":-60,"elapsed":26856,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["trainer = UGATIT(trainA_path='/content/photo_jpg/', resume=True, trainB_path='/content/drive/MyDrive/photo2monet/monetphotos/', name_model='train_epoch_56.pt',\n","                 submissions_path='/content/drive/MyDrive/photo2monet/submissions/epoch56/')\n","trainer.build_model()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"_N8qHv8ugSYf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610098285138,"user_tz":-60,"elapsed":242711,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"aeb20cf2-ccff-4df5-cbaa-b8484c19f404"},"source":["trainer.test() "],"execution_count":12,"outputs":[{"output_type":"stream","text":[" [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7U4kyh8Qm22e"},"source":["num_epochs = ['2', '3', '4', '5', '6', '8', '10', '12'][::-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iz5GFSMbnOem","executionInfo":{"status":"ok","timestamp":1608999500486,"user_tz":-60,"elapsed":1367415,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"b3a17ab5-8802-4eda-f02e-b8fe792ec861"},"source":["for n in num_epochs:\n","  trainer = UGATIT(trainA_path='/content/photo_jpg/', resume=True, trainB_path='/content/drive/MyDrive/photo2monet/monetphotos/', name_model=f'train_epoch_{n}.pt',\n","                 submissions_path=f'/content/drive/MyDrive/photo2monet/submissions/epoch{n}/')\n","  trainer.build_model()\n","  trainer.test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"," [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"," [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"," [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"," [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"," [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"," [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"," [*] Load SUCCESS\n","0 Monet Picture have been generated\n","200 Monet Picture have been generated\n","400 Monet Picture have been generated\n","600 Monet Picture have been generated\n","800 Monet Picture have been generated\n","1000 Monet Picture have been generated\n","1200 Monet Picture have been generated\n","1400 Monet Picture have been generated\n","1600 Monet Picture have been generated\n","1800 Monet Picture have been generated\n","2000 Monet Picture have been generated\n","2200 Monet Picture have been generated\n","2400 Monet Picture have been generated\n","2600 Monet Picture have been generated\n","2800 Monet Picture have been generated\n","3000 Monet Picture have been generated\n","3200 Monet Picture have been generated\n","3400 Monet Picture have been generated\n","3600 Monet Picture have been generated\n","3800 Monet Picture have been generated\n","4000 Monet Picture have been generated\n","4200 Monet Picture have been generated\n","4400 Monet Picture have been generated\n","4600 Monet Picture have been generated\n","4800 Monet Picture have been generated\n","5000 Monet Picture have been generated\n","5200 Monet Picture have been generated\n","5400 Monet Picture have been generated\n","5600 Monet Picture have been generated\n","5800 Monet Picture have been generated\n","6000 Monet Picture have been generated\n","6200 Monet Picture have been generated\n","6400 Monet Picture have been generated\n","6600 Monet Picture have been generated\n","6800 Monet Picture have been generated\n","7000 Monet Picture have been generated\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DOpPLUmjxjPS"},"source":["https://github.com/ranery/Bayesian-CycleGAN\n","\n","https://github.com/henry32144/cyclegan-notebook\n","\n","https://www.kaggle.com/balraj98/monet2photo (more Monet Data)\n","\n","https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf (look into it)"]},{"cell_type":"code","metadata":{"id":"pCQhZrNnElHz"},"source":["trainer = UGATIT(trainA_path='/content/drive/MyDrive/photo2monet/input/', resume=True, trainB_path='/content/drive/MyDrive/photo2monet/monetphotos/', name_model='train_epoch_38.pt',\n","                 submissions_path='/content/drive/MyDrive/photo2monet/submissions/')\n","trainer.build_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYUdoATSFDlJ","executionInfo":{"status":"ok","timestamp":1609591333109,"user_tz":-60,"elapsed":2093,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"b64734cb-39ba-4bb6-ce22-641d4ce09d4b"},"source":["trainer.test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" [*] Load SUCCESS\n","0 Monet Picture have been generated\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SdHmSzjtFIHe"},"source":[""],"execution_count":null,"outputs":[]}]}