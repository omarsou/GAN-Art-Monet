{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BayesianCycleGan.ipynb","provenance":[],"collapsed_sections":["F5jk3g9Jd54L","W14SklNDQRp3","5ssmf_-wEP-L","6pZCuvQFDVjo","f7KroGBaK9r5"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1QPjHoEQYqgFTzeW02bGL1hZt5jEhyFn-","authorship_tag":"ABX9TyNKQEyksBSibbfESeWDjDqt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jYbUnOGavHql"},"source":["Based on the paper : https://ieeexplore.ieee.org/document/9186319\n","\n","Based on the code : https://github.com/ranery/Bayesian-CycleGAN"]},{"cell_type":"code","metadata":{"id":"rWLRypLUdymp"},"source":["!nvidia-smi -L\n","!pip install --upgrade --force-reinstall --no-deps kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RVBG-hkSD4po","executionInfo":{"status":"ok","timestamp":1611509516995,"user_tz":-60,"elapsed":3500,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","from torch.nn.parameter import Parameter\n","import torch.utils.data as data\n","\n","import functools\n","import numpy as np\n","import cv2\n","from scipy import misc\n","import time, itertools\n","import random\n","from collections import OrderedDict\n","\n","from PIL import Image\n","\n","import os\n","import os.path"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5jk3g9Jd54L"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"zbg0j_NSd7Uh","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1611509527465,"user_tz":-60,"elapsed":10462,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"13fddb80-f0cc-4a07-d25d-5cb9c19a6cf9"},"source":["from google.colab import files\n","# Here you should upload your Kaggle API key (see : https://www.kaggle.com/docs/api (Authentification paragraph))\n","files.upload()"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-fcd12c80-676b-4a8c-b8b3-115b45b70c20\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-fcd12c80-676b-4a8c-b8b3-115b45b70c20\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving kaggle.json to kaggle.json\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"negalov\",\"key\":\"f2d706af1447ced98029686098226cba\"}'}"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"AxCtfHOneAH2"},"source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","! kaggle datasets list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gt93ENw1eAxB"},"source":["! kaggle competitions download -c gan-getting-started"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_L190GHeE16"},"source":["! unzip /content/gan-getting-started.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W14SklNDQRp3"},"source":["# Utils"]},{"cell_type":"code","metadata":{"id":"jjkvBxlIPDUK","executionInfo":{"status":"ok","timestamp":1611509550575,"user_tz":-60,"elapsed":793,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["class ImagePool():\n","    def __init__(self, pool_size):\n","        self.pool_size = pool_size\n","        if self.pool_size > 0:\n","            self.num_imgs = 0\n","            self.images = []\n","\n","    def query(self, images):\n","        if self.pool_size == 0:\n","            return Variable(images)\n","        return_images = []\n","        for image in images:\n","            image = torch.unsqueeze(image, 0)\n","            if self.num_imgs < self.pool_size:\n","                self.num_imgs = self.num_imgs + 1\n","                self.images.append(image)\n","                return_images.append(image)\n","            else:\n","                p = random.uniform(0, 1)\n","                if p > 0.5:\n","                    random_id = random.randint(0, self.pool_size-1)\n","                    tmp = self.images[random_id].clone()\n","                    self.images[random_id] = image\n","                    return_images.append(tmp)\n","                else:\n","                    return_images.append(image)\n","        return_images = Variable(torch.cat(return_images, 0))\n","        return return_images\n","\n","def tensor2im(image_tensor, imtype=np.uint8):\n","    image_numpy = image_tensor.detach().cpu().float().numpy()\n","    #if image_numpy.shape[0] == 1:\n","    image_numpy = image_numpy[0]\n","    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n","    return image_numpy.astype(imtype)\n","\n","\n","def diagnose_network(net, name='network'):\n","    mean = 0.0\n","    count = 0\n","    for param in net.parameters():\n","        if param.grad is not None:\n","            mean += torch.mean(torch.abs(param.grad.data))\n","            count += 1\n","    if count > 0:\n","        mean = mean / count\n","    print(name)\n","    print(mean)\n","\n","\n","def save_image(image_numpy, image_path):\n","    image_pil = Image.fromarray(image_numpy)\n","    image_pil.save(image_path)\n","\n","\n","def print_numpy(x, val=True, shp=False):\n","    x = x.astype(np.float64)\n","    if shp:\n","        print('shape,', x.shape)\n","    if val:\n","        x = x.flatten()\n","        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n","            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n","\n","\n","def mkdirs(paths):\n","    if isinstance(paths, list) and not isinstance(paths, str):\n","        for path in paths:\n","            mkdir(path)\n","    else:\n","        mkdir(paths)\n","\n","\n","def mkdir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"8o_56TkTw0hq","executionInfo":{"status":"ok","timestamp":1611509553832,"user_tz":-60,"elapsed":658,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["class Visualizer():\n","    def __init__(self, opt):\n","        # self.opt = opt\n","        self.display_id = opt.display_id\n","        self.use_html = opt.isTrain and not opt.no_html\n","        self.win_size = opt.display_winsize\n","        self.name = opt.name\n","        self.opt = opt\n","        self.saved = False\n","        self.img_dir = os.path.join(opt.checkpoints_dir, opt.name, 'images')\n","        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, 'loss_log.txt')\n","        try:\n","          with open(self.log_name, \"a\") as log_file:\n","            now = time.strftime(\"%c\")\n","            log_file.write('================ Training Loss (%s) ================\\n' % now)\n","        except FileNotFoundError:\n","          with open(self.log_name, \"w\") as log_file:\n","            now = time.strftime(\"%c\")\n","            log_file.write('================ Training Loss (%s) ================\\n' % now)\n","\n","    def reset(self):\n","        self.saved = False\n","\n","    # |visuals|: dictionary of images to display or save\n","    def display_current_results(self, visuals, epoch, save_result):\n","        for label, image_numpy in visuals.items():\n","            img_path = os.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))\n","            save_image(image_numpy, img_path)\n","\n","    # errors: same format as |errors| of plotCurrentErrors\n","    def print_current_errors(self, epoch, i, errors, t):\n","        message = '(epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)\n","        for k, v in errors.items():\n","            message += '%s: %.3f ' % (k, v)\n","        print(message)\n","        with open(self.log_name, \"a\") as log_file:\n","            log_file.write('%s\\n' % message)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ssmf_-wEP-L"},"source":["# Networks"]},{"cell_type":"code","metadata":{"id":"fAYOjuf-ERTf","executionInfo":{"status":"ok","timestamp":1611509560254,"user_tz":-60,"elapsed":974,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["class GANLoss(nn.Module):\n","    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n","                 tensor=torch.FloatTensor):\n","        super(GANLoss, self).__init__()\n","        self.real_label = target_real_label\n","        self.fake_label = target_fake_label\n","        self.real_label_var = None\n","        self.fake_label_var = None\n","        self.Tensor = tensor\n","        if use_lsgan:\n","            self.loss = nn.MSELoss()\n","        else:\n","            self.loss = nn.BCELoss()\n","\n","    def get_target_tensor(self, input, target_is_real):\n","        target_tensor = None\n","        if target_is_real:\n","            create_label = ((self.real_label_var is None) or\n","                            (self.real_label_var.numel() != input.numel()))\n","            if create_label:\n","                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n","                self.real_label_var = Variable(real_tensor, requires_grad=False)\n","            target_tensor = self.real_label_var\n","        else:\n","            create_label = ((self.fake_label_var is None) or\n","                            (self.fake_label_var.numel() != input.numel()))\n","            if create_label:\n","                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n","                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n","            target_tensor = self.fake_label_var\n","        return target_tensor\n","\n","    def __call__(self, input, target_is_real):\n","        if isinstance(input[0], list):\n","            loss = 0\n","            for input_i in input:\n","                pred = input_i[-1]\n","                target_tensor = self.get_target_tensor(pred, target_is_real)\n","                loss += self.loss(pred, target_tensor)\n","            return loss\n","        else:\n","            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n","            return self.loss(input[-1], target_tensor)\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXv9nTfzGEeZ","executionInfo":{"status":"ok","timestamp":1611509562518,"user_tz":-60,"elapsed":984,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["class ResnetBlock(nn.Module):\n","    def __init__(self, dim, padding_type, norm_layer, activation=nn.ReLU(True), use_dropout=False):\n","        super(ResnetBlock, self).__init__()\n","        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n","\n","    def build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n","        conv_block = []\n","        p = 0\n","        if padding_type == 'reflect':\n","            conv_block += [nn.ReflectionPad2d(1)]\n","        elif padding_type == 'replicate':\n","            conv_block += [nn.ReplicationPad2d(1)]\n","        elif padding_type == 'zero':\n","            p = 1\n","        else:\n","            raise NotImplementedError('padding [%s] is not implemented!' % padding_type)\n","\n","        conv_block += [\n","            nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n","            norm_layer(dim),\n","            activation,\n","        ]\n","\n","        if use_dropout:\n","            conv_block += [nn.Dropout(0.5)]\n","\n","        p = 0\n","        if padding_type == 'reflect':\n","            conv_block += [nn.ReflectionPad2d(1)]\n","        elif padding_type == 'replicate':\n","            conv_block += [nn.ReplicationPad2d(1)]\n","        elif padding_type == 'zero':\n","            p = 1\n","        else:\n","            raise NotImplementedError('padding [%s] is not implemented!' % padding_type)\n","\n","        conv_block += [\n","            nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n","            norm_layer(dim),\n","            activation,\n","        ]\n","\n","        return nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        out = x + self.conv_block(x)\n","        return out\n","\n","class GlobalGenerator(nn.Module):\n","    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9,\n","                 norm_layer=nn.BatchNorm2d, padding_type='reflect'):\n","        assert(n_blocks >= 0)\n","        super(GlobalGenerator, self).__init__()\n","        activation = nn.ReLU(True)\n","\n","        model = [\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n","            norm_layer(ngf),\n","            activation,\n","        ]\n","\n","        # downsample\n","        for i in range(n_downsampling):\n","            mult = 2**i\n","            model += [\n","                nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n","                norm_layer(ngf * mult * 2),\n","                activation,\n","            ]\n","\n","        # resnet blocks\n","        mult = 2**n_downsampling\n","        for i in range(n_blocks):\n","            model += [\n","                ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)\n","            ]\n","\n","        # upsample\n","        for i in range(n_downsampling):\n","            mult = 2**(n_downsampling - i)\n","            model += [\n","                nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n","                norm_layer(int(ngf * mult / 2)),\n","                activation,\n","            ]\n","        model += [\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n","            nn.Tanh(),\n","        ]\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input):\n","        return self.model(input)\n","\n","class MultiscaleDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, num_D=3, use_dropout=False):\n","        super(MultiscaleDiscriminator, self).__init__()\n","        self.num_D = num_D\n","        self.n_layers = n_layers\n","\n","        for i in range(num_D):\n","            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, use_dropout)\n","            setattr(self, 'layer'+str(i), netD.model)\n","\n","        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n","\n","    def singleD_forward(self, model, input):\n","        return [model(input)]\n","\n","    def forward(self, input):\n","        num_D = self.num_D\n","        result = []\n","        input_downsampled = input\n","        for i in range(num_D):\n","            model = getattr(self, 'layer'+str(num_D-1-i))\n","            result.append(self.singleD_forward(model, input_downsampled))\n","            if i != (num_D-1):\n","                input_downsampled = self.downsample(input_downsampled)\n","        return result\n","\n","class NLayerDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, use_dropout=False):\n","        super(NLayerDiscriminator, self).__init__()\n","        self.n_layers = n_layers\n","\n","        kw = 4\n","        padw = int(np.ceil((kw-1.0)/2))\n","        model = [\n","            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        nf = ndf\n","        for n in range(1, n_layers):\n","            nf_prev = nf\n","            nf = min(nf * 2, 512)\n","            model += [\n","                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n","                norm_layer(nf),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        nf_prev = nf\n","        nf = min(nf * 2, 512)\n","        model += [\n","            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n","            norm_layer(nf),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","        model += [nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]\n","\n","        if use_sigmoid:\n","            model += [nn.Sigmoid()]\n","\n","        if use_dropout:\n","            model = model + [nn.Dropout(0.5)]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input):\n","        return self.model(input)\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_nc, output_nc, ngf=64, n_layers=4, norm_layer=nn.BatchNorm2d, ratio=1):\n","        super(Encoder, self).__init__()\n","        self.output_nc = output_nc\n","\n","        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n","                 norm_layer(ngf), nn.ReLU(True)]\n","        ### downsample\n","        for i in range(n_layers):\n","            mult = 2**i\n","            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n","                      norm_layer(ngf * mult * 2), nn.ReLU(True)]\n","\n","        self.downsample = nn.Sequential(*model)\n","        self.pool = nn.AvgPool2d(32)\n","        self.fc = nn.Sequential(*[nn.Linear(int(ngf * mult * 2 * 4 / ratio), 32)])\n","        self.fcVar = nn.Sequential(*[nn.Linear(int(ngf * mult * 2 * 4 / ratio), 32)])\n","\n","        ### upsample\n","        for i in range(n_layers):\n","            mult = 2**(n_layers - i)\n","            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n","                       norm_layer(int(ngf * mult / 2)), nn.ReLU(True)]\n","\n","        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input):\n","        feature = self.model(input)\n","        _conv = self.downsample(input)\n","        _conv = self.pool(_conv)\n","        # print(_conv)\n","        _conv = _conv.view(input.size(0), -1)\n","        #print(_conv.shape)\n","        output = self.fc(_conv)\n","        outputVar = self.fcVar(_conv)\n","        return output, outputVar, feature"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KgjLzXrE3xn","executionInfo":{"status":"ok","timestamp":1611509566338,"user_tz":-60,"elapsed":1070,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["def weights_init_gaussian(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('Linear') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm2d') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","def weights_init_uniform(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        init.uniform(m.weight.data, -0.06, 0.06)\n","    elif classname.find('Conv') != -1:\n","        init.uniform(m.weight.data, -0.06, 0.06)\n","    elif classname.find('BatchNorm2d') != -1:\n","        init.uniform(m.weight.data, 0.04, 1.06)\n","        init.constant(m.bias.data, 0.0)\n","\n","def get_norm_layer():\n","    norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n","    return norm_layer\n","\n","def define_G(input_nc, output_nc, ngf, netG, n_downsample_global=3, n_blocks_global=9, norm='instance', ratio=1):\n","    norm_layer = get_norm_layer()\n","    if netG == 'global':\n","        netG = GlobalGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)\n","    elif netG == 'encoder':\n","        netG = Encoder(input_nc, output_nc, 64, n_downsample_global, norm_layer, ratio)\n","    else:\n","        raise NotImplementedError('generator [%s] is not found.' % netG)\n","    netG.apply(weights_init_gaussian)\n","    return netG\n","\n","def define_D(input_nc, ndf, n_layers_D, norm='instance', use_sigmoid=False, num_D=1):\n","    norm_layer = get_norm_layer()\n","    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, use_dropout=False)\n","    netD.apply(weights_init_gaussian)\n","    return netD"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pZCuvQFDVjo"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"UX4CxjXBOJrx","executionInfo":{"status":"ok","timestamp":1611509577059,"user_tz":-60,"elapsed":3175,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","class CycleGAN():\n","    def name(self):\n","        return 'Bayesian CycleGAN Model'\n","\n","    def initialize(self, opt):\n","        self.opt = opt\n","        self.isTrain = opt.isTrain\n","        if torch.cuda.is_available():\n","            print('cuda is available, we will use gpu!')\n","            self.Tensor = torch.cuda.FloatTensor\n","            #self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","            torch.cuda.manual_seed_all(100)\n","        else:\n","            self.Tensor = torch.FloatTensor\n","            torch.manual_seed(100)\n","        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n","\n","        # get radio for network initialization\n","        ratio = 256 * 256 / opt.loadSize / (opt.loadSize / opt.ratio)\n","\n","        # load network\n","        netG_input_nc = opt.input_nc + 1\n","        netG_output_nc = opt.output_nc + 1\n","        self.netG_A = define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG_A,\n","                                        opt.n_downsample_global, opt.n_blocks_global, opt.norm).type(self.Tensor)#.cuda()\n","        self.netG_B = define_G(netG_output_nc, opt.input_nc, opt.ngf, opt.netG_B,\n","                                        opt.n_downsample_global, opt.n_blocks_global, opt.norm).type(self.Tensor)#.cuda()\n","\n","        self.netE_A = define_G(opt.input_nc, 1, 64, 'encoder', opt.n_downsample_global, norm=opt.norm, ratio=ratio).type(self.Tensor)#.cuda()\n","        self.netE_B = define_G(opt.output_nc, 1, 64, 'encoder', opt.n_downsample_global, norm=opt.norm, ratio=ratio).type(self.Tensor)#.cuda()\n","\n","        if self.isTrain:\n","            use_sigmoid = opt.no_lsgan\n","            self.netD_A = define_D(opt.output_nc, opt.ndf, opt.n_layers_D, opt.norm,\n","                                            use_sigmoid, opt.num_D_A).type(self.Tensor)#.cuda()\n","            self.netD_B = define_D(opt.input_nc, opt.ndf, opt.n_layers_D, opt.norm,\n","                                            use_sigmoid, opt.num_D_B).type(self.Tensor)#.cuda()\n","\n","        if not self.isTrain or opt.continue_train:\n","            self.load_network(self.netG_A, 'G_A', opt.which_epoch, self.save_dir)\n","            self.load_network(self.netG_B, 'G_B', opt.which_epoch, self.save_dir)\n","            self.load_network(self.netE_A, 'E_A', opt.which_epoch, self.save_dir)\n","            self.load_network(self.netE_B, 'E_B', opt.which_epoch, self.save_dir)\n","            if self.isTrain:\n","                self.load_network(self.netD_A, 'D_A', opt.which_epoch, self.save_dir)\n","                self.load_network(self.netD_B, 'D_B', opt.which_epoch, self.save_dir)\n","\n","        # set loss functions and optimizers\n","        if self.isTrain:\n","            self.old_lr = opt.lr\n","            # define loss function\n","            self.criterionGAN = GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n","            self.criterionCycle = torch.nn.L1Loss()\n","            self.criterionL1 = torch.nn.L1Loss()\n","            # initialize optimizers\n","            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_E_A = torch.optim.Adam(self.netE_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_E_B = torch.optim.Adam(self.netE_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n","\n","        print('Network initialized!')\n","\n","        # dataset path and name list\n","        self.origin_path = os.getcwd()\n","        self.path_A = self.opt.dataroot_A\n","        self.path_B = self.opt.dataroot_B\n","        self.list_A = os.listdir(self.path_A)\n","        self.list_B = os.listdir(self.path_B)\n","\n","    def set_input(self, input):\n","        AtoB = self.opt.which_direction == 'AtoB'\n","        self.input_A = input['A' if AtoB else 'B']\n","        self.input_B = input['B' if AtoB else 'A']\n","        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n","\n","    def forward(self):\n","        self.real_A = Variable(self.input_A).type(self.Tensor)#.cuda()\n","        self.real_B = Variable(self.input_B).type(self.Tensor)#.cuda()\n","\n","        # feature map\n","        mc_sample_x = random.sample(self.list_A, self.opt.mc_x)\n","        mc_sample_y = random.sample(self.list_B, self.opt.mc_y)\n","        self.real_B_zx = []\n","        self.real_A_zy = []\n","        self.mu_x = []\n","        self.mu_y = []\n","        self.logvar_x = []\n","        self.logvar_y = []\n","        os.chdir(self.path_A)\n","        for sample_x in mc_sample_x:\n","            z_x = Image.open(sample_x).convert('RGB')\n","            z_x = self.img_resize(z_x, self.opt.loadSize)\n","            z_x = transform(z_x)\n","            if self.opt.input_nc == 1:  # RGB to gray\n","                z_x = z_x[0, ...] * 0.299 + z_x[1, ...] * 0.587 + z_x[2, ...] * 0.114\n","                z_x = z_x.unsqueeze(0)\n","            z_x = Variable(z_x).type(self.Tensor)#.cuda()\n","            z_x = torch.unsqueeze(z_x, 0)\n","            mu_x, logvar_x, feat_map = self.netE_A.forward(z_x)\n","            self.mu_x.append(mu_x)\n","            self.logvar_x.append(logvar_x)\n","            self.feat_map_zx = feat_map\n","            real_B_zx = []\n","            for i in range(0, self.opt.batchSize):\n","                _real = torch.unsqueeze(self.real_B[i], 0)\n","                _real = torch.cat([_real, feat_map], dim=1)\n","                real_B_zx.append(_real)\n","            real_B_zx = torch.cat(real_B_zx)\n","            self.real_B_zx.append(real_B_zx)\n","        self.mu_x = torch.cat(self.mu_x)\n","        self.logvar_x = torch.cat(self.logvar_x)\n","\n","        os.chdir(self.path_B)\n","        for sample_y in mc_sample_y:\n","            z_y = Image.open(sample_y).convert('RGB')\n","            z_y = self.img_resize(z_y, self.opt.loadSize)\n","            z_y = transform(z_y)\n","            if self.opt.output_nc == 1:  # RGB to gray\n","                z_y = z_y[0, ...] * 0.299 + z_y[1, ...] * 0.587 + z_y[2, ...] * 0.114\n","                z_y = z_y.unsqueeze(0)\n","            z_y = Variable(z_y).type(self.Tensor)#.cuda()\n","            z_y = torch.unsqueeze(z_y, 0)\n","\n","            mu_y, logvar_y, feat_map = self.netE_B.forward(z_y)\n","            self.mu_y.append(mu_y)\n","            self.logvar_y.append(logvar_y)\n","            self.feat_map_zy = feat_map\n","            real_A_zy = []\n","            for i in range(0, self.opt.batchSize):\n","                _real = torch.unsqueeze(self.real_A[i], 0)\n","                _real = torch.cat((_real, feat_map), dim=1)\n","                real_A_zy.append(_real)\n","            real_A_zy = torch.cat(real_A_zy)\n","            self.real_A_zy.append(real_A_zy)\n","        self.mu_y = torch.cat(self.mu_y)\n","        self.logvar_y = torch.cat(self.logvar_y)\n","\n","        os.chdir(self.origin_path)\n","\n","    def inference(self):\n","        real_A = Variable(self.input_A).type(self.Tensor)\n","        real_B = Variable(self.input_B).type(self.Tensor)\n","\n","        # feature map\n","        os.chdir(self.path_A)\n","        mc_sample_x = random.sample(self.list_A, 1)\n","        z_x = Image.open(mc_sample_x[0]).convert('RGB')\n","        z_x = self.img_resize(z_x, self.opt.loadSize)\n","        z_x = transform(z_x)\n","        if self.opt.input_nc == 1:  # RGB to gray\n","            z_x = z_x[0, ...] * 0.299 + z_x[1, ...] * 0.587 + z_x[2, ...] * 0.114\n","            z_x = z_x.unsqueeze(0)\n","        if self.opt.use_feat:\n","            z_x = z_x[0, ...] * 0.299 + z_x[1, ...] * 0.587 + z_x[2, ...] * 0.114\n","            z_x = z_x.unsqueeze(0)\n","        z_x = Variable(z_x).type(self.Tensor)\n","        z_x = torch.unsqueeze(z_x, 0)\n","\n","        if not self.opt.use_feat:\n","            mu_x, logvar_x, feat_map_zx = self.netE_A.forward(z_x)\n","        else:\n","            feat_map_zx = z_x\n","\n","        os.chdir(self.path_B)\n","        mc_sample_y = random.sample(self.list_B, 1)\n","        z_y = Image.open(mc_sample_y[0]).convert('RGB')\n","        z_y = self.img_resize(z_y, self.opt.loadSize)\n","        z_y = transform(z_y)\n","        if self.opt.output_nc == 1:  # RGB to gray\n","            z_y = z_y[0, ...] * 0.299 + z_y[1, ...] * 0.587 + z_y[2, ...] * 0.114\n","            z_y = z_y.unsqueeze(0)\n","        if self.opt.use_feat:\n","            z_y = z_y[0, ...] * 0.299 + z_y[1, ...] * 0.587 + z_y[2, ...] * 0.114\n","            z_y = z_y.unsqueeze(0)\n","        z_y = Variable(z_y).type(self.Tensor)\n","        z_y = torch.unsqueeze(z_y, 0)\n","\n","        if not self.opt.use_feat:\n","            mu_y, logvar_y, feat_map_zy = self.netE_B.forward(z_y)\n","        else:\n","            feat_map_zy = z_y\n","\n","        os.chdir(self.origin_path)\n","\n","        # combine input image with random feature map\n","        real_B_zx = []\n","        for i in range(0, self.opt.batchSize):\n","            _real = torch.cat((real_B[i:i+1], feat_map_zx), dim=1)\n","            real_B_zx.append(_real)\n","        real_B_zx = torch.cat(real_B_zx)\n","        real_A_zy = []\n","        for i in range(0, self.opt.batchSize):\n","            _real = torch.cat((real_A[i:i+1], feat_map_zy), dim=1)\n","            real_A_zy.append(_real)\n","        real_A_zy = torch.cat(real_A_zy)\n","\n","        # inference\n","        fake_B = self.netG_A(real_A_zy)\n","        fake_B_next = torch.cat((fake_B, feat_map_zx), dim=1)\n","        self.rec_A = self.netG_B(fake_B_next)\n","        self.fake_B = fake_B\n","\n","        fake_A = self.netG_B(real_B_zx)\n","        fake_A_next = torch.cat((fake_A, feat_map_zy), dim=1)\n","        self.rec_B = self.netG_A(fake_A_next)\n","        self.fake_A = fake_A\n","\n","    def get_image_paths(self):\n","        return self.image_paths\n","\n","    def img_resize(self, img, target_width):\n","        ow, oh = img.size\n","        if (ow == target_width):\n","            return img\n","        else:\n","            w = target_width\n","            h = int(target_width * oh / ow)\n","        return img.resize((w, h), Image.BICUBIC)\n","\n","    def get_z_random(self, batchSize, nz, random_type='gauss'):\n","        z = self.Tensor(batchSize, nz)\n","        if random_type == 'uni':\n","            z.copy_(torch.rand(batchSize, nz) * 2.0 - 1.0)\n","        elif random_type == 'gauss':\n","            z.copy_(torch.randn(batchSize, nz))\n","        z = Variable(z)\n","        return z\n","\n","    def backward_G(self):\n","        # GAN loss D_A(G_A(A))\n","        fake_B = []\n","        for real_A in self.real_A_zy:\n","            _fake = self.netG_A(real_A)\n","            fake_B.append(_fake)\n","        fake_B = torch.cat(fake_B)\n","\n","        pred_fake = self.netD_A(fake_B)\n","        loss_G_A = self.criterionGAN(pred_fake, True)\n","\n","        # GAN loss D_B(G_B(B))\n","        fake_A = []\n","        for real_B in self.real_B_zx:\n","            _fake = self.netG_B(real_B)\n","            fake_A.append(_fake)\n","        fake_A = torch.cat(fake_A)\n","\n","        pred_fake = self.netD_B(fake_A)\n","        loss_G_B = self.criterionGAN(pred_fake, True)\n","\n","        # cycle loss\n","        lambda_A = self.opt.lambda_A\n","        lambda_B = self.opt.lambda_B\n","\n","        # Forward cycle loss\n","        fake_B_next = []\n","        for i in range(0, fake_B.size(0)):\n","        \t_fake = fake_B[i:(i+1)]\n","        \t_fake = torch.cat((_fake, self.feat_map_zx), dim=1)\n","        \tfake_B_next.append(_fake)\n","        fake_B_next = torch.cat(fake_B_next)\n","\n","        rec_A = self.netG_B(fake_B_next)\n","        loss_cycle_A = 0\n","        for i in range(0, self.opt.mc_y):\n","            loss_cycle_A += self.criterionCycle(rec_A[i*self.real_A.size(0):(i+1)*self.real_A.size(0)], self.real_A) * lambda_A\n","        pred_cycle_G_A = self.netD_B(rec_A)\n","        loss_cycle_G_A = self.criterionGAN(pred_cycle_G_A, True)\n","\n","        # Backward cycle loss\n","        fake_A_next = []\n","        for i in range(0, fake_A.size(0)):\n","        \t_fake = fake_A[i:(i+1)]\n","        \t_fake = torch.cat((_fake, self.feat_map_zy), dim=1)\n","        \tfake_A_next.append(_fake)\n","        fake_A_next = torch.cat(fake_A_next)\n","\n","        rec_B = self.netG_A(fake_A_next)\n","        loss_cycle_B = 0\n","        for i in range(0, self.opt.mc_x):\n","            loss_cycle_B += self.criterionCycle(rec_B[i*self.real_B.size(0):(i+1)*self.real_B.size(0)], self.real_B) * lambda_B\n","        pred_cycle_G_B = self.netD_A(rec_B)\n","        loss_cycle_G_B = self.criterionGAN(pred_cycle_G_B, True)\n","\n","        # prior loss\n","        prior_loss_G_A = self.get_prior(self.netG_A.parameters(), self.opt.batchSize)\n","        prior_loss_G_B = self.get_prior(self.netG_B.parameters(), self.opt.batchSize)\n","\n","        # KL loss\n","        kl_element = self.mu_x.pow(2).add_(self.logvar_x.exp()).mul_(-1).add_(1).add_(self.logvar_x)\n","        loss_kl_EA = torch.sum(kl_element).mul_(-0.5) * self.opt.lambda_kl\n","\n","        kl_element = self.mu_y.pow(2).add_(self.logvar_y.exp()).mul_(-1).add_(1).add_(self.logvar_y)\n","        loss_kl_EB = torch.sum(kl_element).mul_(-0.5) * self.opt.lambda_kl\n","\n","        # total loss\n","        loss_G =  loss_G_A + loss_G_B + (prior_loss_G_A + prior_loss_G_B) + (loss_cycle_G_A + loss_cycle_G_B) * self.opt.gamma + (loss_cycle_A + loss_cycle_B) + (loss_kl_EA + loss_kl_EB)\n","        loss_G.backward()\n","\n","        self.fake_B = fake_B\n","        self.fake_A = fake_A\n","        self.rec_A = rec_A\n","        self.rec_B = rec_B\n","\n","        self.loss_G_A = loss_G_A.item() + loss_cycle_G_A.item() * self.opt.gamma + prior_loss_G_A.item()\n","        self.loss_G_B = loss_G_B.item() + loss_cycle_G_B.item() * self.opt.gamma + prior_loss_G_A.item()\n","        self.loss_cycle_A = loss_cycle_A.item()\n","        self.loss_cycle_B = loss_cycle_B.item()\n","        self.loss_kl_EA = loss_kl_EA.item()\n","        self.loss_kl_EB = loss_kl_EB.item()\n","\n","    def backward_D_A(self):\n","        fake_B = Variable(self.fake_B).type(self.Tensor)#.cuda()\n","        rec_B = Variable(self.rec_B).type(self.Tensor)#.cuda()\n","        # how well it classifiers fake images\n","        pred_fake = self.netD_A(fake_B.detach())\n","        loss_D_fake = self.criterionGAN(pred_fake, False)\n","        pred_cycle_fake = self.netD_A(rec_B.detach())\n","        loss_D_cycle_fake = self.criterionGAN(pred_cycle_fake, False)\n","\n","        # how well it classifiers real images\n","        pred_real = self.netD_A(self.real_B)\n","        loss_D_real = self.criterionGAN(pred_real, True) * self.opt.mc_y\n","\n","        # prior loss\n","        prior_loss_D_A = self.get_prior(self.netD_A.parameters(), self.opt.batchSize)\n","\n","        # total loss\n","        loss_D_A = (loss_D_real + loss_D_fake) * 0.5 + (loss_D_real + loss_D_cycle_fake) * 0.5 * self.opt.gamma + prior_loss_D_A\n","        loss_D_A.backward()\n","        self.loss_D_A = loss_D_A.item()\n","\n","    def backward_D_B(self):\n","        fake_A = Variable(self.fake_A).type(self.Tensor)#.cuda()\n","        rec_A = Variable(self.rec_A).type(self.Tensor)#.cuda()\n","        # how well it classifiers fake images\n","        pred_fake = self.netD_B(fake_A.detach())\n","        loss_D_fake = self.criterionGAN(pred_fake, False)\n","        pred_cycle_fake = self.netD_B(rec_A.detach())\n","        loss_D_cycle_fake = self.criterionGAN(pred_cycle_fake, False)\n","\n","        # how well it classifiers real images\n","        pred_real = self.netD_B(self.real_A)\n","        loss_D_real = self.criterionGAN(pred_real, True) * self.opt.mc_x\n","\n","        # prior loss\n","        prior_loss_D_B = self.get_prior(self.netD_B.parameters(), self.opt.batchSize)\n","\n","        # total loss\n","        loss_D_B = (loss_D_real + loss_D_fake) * 0.5 + (loss_D_real + loss_D_cycle_fake) * 0.5 * self.opt.gamma + prior_loss_D_B\n","        loss_D_B.backward()\n","        self.loss_D_B = loss_D_B.item()\n","\n","\n","    def optimize(self):\n","        # forward\n","        self.forward()\n","        # G_A and G_B\n","        # E_A and E_B\n","        self.optimizer_G.zero_grad()\n","        self.optimizer_E_A.zero_grad()\n","        self.optimizer_E_B.zero_grad()\n","\n","        self.backward_G()\n","        \n","        self.optimizer_G.step()\n","        self.optimizer_E_A.step()\n","        self.optimizer_E_B.step()\n","        # D_A\n","        self.optimizer_D_A.zero_grad()\n","\n","        self.backward_D_A()\n","\n","        self.optimizer_D_A.step()\n","        # D_B\n","        self.optimizer_D_B.zero_grad()\n","\n","        self.backward_D_B()\n","        self.optimizer_D_B.step()\n","\n","    def get_current_loss(self):\n","        loss = OrderedDict([\n","            ('D_A', self.loss_D_A),\n","            ('D_B', self.loss_D_B),\n","            ('G_A', self.loss_G_A),\n","            ('G_B', self.loss_G_B)\n","        ])\n","        if self.opt.gamma == 0:\n","            loss['cyc_A'] = self.loss_cycle_A\n","            loss['cyc_B'] = self.loss_cycle_B\n","        elif self.opt.gamma > 0:\n","            loss['cyc_G_A'] = self.loss_cycle_A\n","            loss['cyc_G_B'] = self.loss_cycle_B\n","        if self.opt.lambda_kl > 0:\n","        \tloss['kl_EA'] = self.loss_kl_EA\n","        \tloss['kl_EB'] = self.loss_kl_EB\n","        return loss\n","\n","    def get_stye_loss(self):\n","        loss = OrderedDict([\n","            ('L1_A', self.loss_G_A_L1),\n","            ('L1_B', self.loss_G_B_L1)\n","        ])\n","        return loss\n","\n","    def get_current_visuals(self):\n","        real_A = tensor2im(self.input_A)\n","        fake_B = tensor2im(self.fake_B)\n","        rec_A = tensor2im(self.rec_A)\n","        real_B = tensor2im(self.input_B)\n","        fake_A = tensor2im(self.fake_A)\n","        rec_B = tensor2im(self.rec_B)\n","        visuals = OrderedDict([\n","            ('real_A', real_A),\n","            ('fake_B', fake_B),\n","            ('rec_A', rec_A),\n","            ('real_B', real_B),\n","            ('fake_A', fake_A),\n","            ('rec_B', rec_B)\n","        ])\n","        return visuals\n","\n","    def get_prior(self, parameters, dataset_size):\n","        prior_loss = Variable(torch.zeros((1))).cuda()\n","        for param in parameters:\n","            prior_loss += torch.mean(param*param)\n","        return prior_loss / dataset_size\n","\n","    def save_model(self, label):\n","        self.save_network(self.netG_A, 'G_A', label)\n","        self.save_network(self.netG_B, 'G_B', label)\n","        self.save_network(self.netE_A, 'E_A', label)\n","        self.save_network(self.netE_B, 'E_B', label)\n","        self.save_network(self.netD_A, 'D_A', label)\n","        self.save_network(self.netD_B, 'D_B', label)\n","\n","    def load_network(self, network, network_label, epoch_label, save_dir=''):\n","        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n","        save_path = os.path.join(self.save_dir, save_filename)\n","        try:\n","            network.load_state_dict(torch.load(save_path))\n","        except:\n","            pretrained_dict = torch.load(save_path)\n","            model_dict = network.state_dict()\n","            try:\n","                pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n","                network.load_state_dict(pretrained_dict)\n","                print('Pretrained network %s has excessive layers; Only loading layers that are used' % network_label)\n","            except:\n","                print('Pretrained network %s has fewer layers; The following are not initialized:' % network_label)\n","                if sys.version_info >= (3, 0):\n","                    not_initialized = set()\n","                else:\n","                    from sets import Set\n","                    not_initialized = Set()\n","                for k, v in pretrained_dict.items():\n","                    if v.size() == model_dict[k].size():\n","                        model_dict[k] = v\n","\n","                for k, v in model_dict.items():\n","                    if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n","                        not_initialized.add(k.split('.')[0])\n","                print(sorted(not_initialized))\n","                network.load_state_dict(model_dict)\n","\n","    def save_network(self, network, network_label, epoch_label):\n","        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n","        save_path = os.path.join(self.save_dir, save_filename)\n","        torch.save(network.cpu().state_dict(), save_path)\n","        if torch.cuda.is_available():\n","            network.cuda()\n","\n","    def print_network(self, net):\n","        num_params = 0\n","        for param in net.parameters():\n","            num_params += param.numel()\n","        print(net)\n","        print('Total number of parameters: %d' % num_params)\n","\n","    # update learning rate (called once every iter)\n","    def update_learning_rate(self, epoch, epoch_iter, dataset_size):\n","        # lrd = self.opt.lr / self.opt.niter_decay\n","        if epoch > self.opt.niter:\n","            lr = self.opt.lr * np.exp(-1.0 * min(1.0, epoch_iter/float(dataset_size)))\n","            for param_group in self.optimizer_D_A.param_groups:\n","                param_group['lr'] = lr\n","            for param_group in self.optimizer_D_B.param_groups:\n","                param_group['lr'] = lr\n","            for param_group in self.optimizer_G.param_groups:\n","                param_group['lr'] = lr\n","            print('update learning rate: %f -> %f' % (self.old_lr, lr))\n","            self.old_lr = lr\n","        else:\n","            lr = self.old_lr"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7KroGBaK9r5"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"DODWhJp1QBpe","executionInfo":{"status":"ok","timestamp":1611509579162,"user_tz":-60,"elapsed":703,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["IMG_EXTENSIONS = [\n","    '.jpg', '.JPG', '.jpeg', '.JPEG',\n","    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n","]\n","\n","class BaseDataLoader():\n","    def __init__(self):\n","        pass\n","    \n","    def initialize(self, opt):\n","        self.opt = opt\n","        pass\n","\n","    def load_data():\n","        return None\n","\n","class BaseDataset(data.Dataset):\n","    def __init__(self):\n","        super(BaseDataset, self).__init__()\n","\n","    def name(self):\n","        return 'BaseDataset'\n","\n","    def initialize(self, opt):\n","        pass\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n","\n","\n","def make_dataset(dir):\n","    images = []\n","    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n","\n","    for root, _, fnames in sorted(os.walk(dir)):\n","        for fname in fnames:\n","            if is_image_file(fname):\n","                path = os.path.join(root, fname)\n","                images.append(path)\n","\n","    return images\n","\n","def get_transform(opt):\n","    transform_list = []\n","    if opt.resize_or_crop == 'resize':         # 1024 x 1024\n","        osize = [opt.loadSize, opt.loadSize]\n","        transform_list.append(transforms.Scale(osize, Image.BICUBIC))\n","        # transform_list.append(transforms.RandomCrop(opt.fineSize))\n","    elif opt.resize_or_crop == 'crop':\n","        transform_list.append(transforms.RandomCrop(opt.fineSize))\n","    elif opt.resize_or_crop == 'scale_width':  # 1024 x 512\n","        transform_list.append(transforms.Lambda(\n","            lambda img: __scale_width(img, opt.loadSize)))\n","    elif opt.resize_or_crop == 'scale_width_and_crop':\n","        transform_list.append(transforms.Lambda(\n","            lambda img: __scale_width(img, opt.loadSize)))\n","        transform_list.append(transforms.RandomCrop(opt.fineSize))\n","\n","    if opt.isTrain and not opt.no_flip:\n","        transform_list.append(transforms.RandomHorizontalFlip())\n","\n","    transform_list += [transforms.ToTensor(),\n","                       transforms.Normalize((0.5, 0.5, 0.5),\n","                                            (0.5, 0.5, 0.5))]\n","    return transforms.Compose(transform_list)\n","\n","def __scale_width(img, target_width):\n","    ow, oh = img.size\n","    if (ow == target_width):\n","        return img\n","    else:\n","        w = target_width\n","        h = int(target_width * oh / ow)\n","    return img.resize((w, h), Image.BICUBIC)\n","\n","class UnalignedDataset(BaseDataset):\n","    def initialize(self, opt):\n","        self.opt = opt\n","        self.dir_A = opt.dataroot_A\n","        self.dir_B = opt.dataroot_B\n","\n","        self.A_paths = make_dataset(self.dir_A)\n","        self.B_paths = make_dataset(self.dir_B)\n","\n","        self.A_paths = sorted(self.A_paths)\n","        self.B_paths = sorted(self.B_paths)\n","        self.A_size = len(self.A_paths)\n","        self.B_size = len(self.B_paths)\n","        self.transform = get_transform(opt)\n","\n","    def __getitem__(self, index):\n","        A_path = self.A_paths[index % self.A_size]\n","        index_A = index % self.A_size\n","        if self.opt.serial_batches:\n","            index_B = index % self.B_size\n","        else:\n","            index_B = random.randint(0, self.B_size - 1)\n","        B_path = self.B_paths[index_B]\n","        # print('(A, B) = (%d, %d)' % (index_A, index_B))\n","        A_img = Image.open(A_path).convert('RGB')\n","        B_img = Image.open(B_path).convert('RGB')\n","\n","        A = self.transform(A_img)\n","        B = self.transform(B_img)\n","        if self.opt.which_direction == 'BtoA':\n","            input_nc = self.opt.output_nc\n","            output_nc = self.opt.input_nc\n","        else:\n","            input_nc = self.opt.input_nc\n","            output_nc = self.opt.output_nc\n","\n","        if input_nc == 1:  # RGB to gray\n","            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n","            A = tmp.unsqueeze(0)\n","\n","        if output_nc == 1:  # RGB to gray\n","            tmp = B[0, ...] * 0.299 + B[1, ...] * 0.587 + B[2, ...] * 0.114\n","            B = tmp.unsqueeze(0)\n","        return {'A': A, 'B': B,\n","                'A_paths': A_path, 'B_paths': B_path}\n","\n","    def __len__(self):\n","        return max(self.A_size, self.B_size)\n","\n","    def name(self):\n","        return 'UnalignedDataset'\n","\n","def CreateDataset(opt):\n","    dataset = None\n","    dataset = UnalignedDataset()\n","    print(\"dataset [%s] was created\" % (dataset.name()))\n","    dataset.initialize(opt)\n","    # dataset.__getitem__(1)\n","    return dataset\n","\n","class CustomDatasetDataLoader(BaseDataLoader):\n","    def name(self):\n","        return 'CustomDatasetDataLoader'\n","\n","    def initialize(self, opt):\n","        BaseDataLoader.initialize(self, opt)\n","        self.dataset = CreateDataset(opt)\n","        self.dataloader = torch.utils.data.DataLoader(\n","            self.dataset,\n","            batch_size=opt.batchSize,\n","            shuffle=not opt.serial_batches,\n","            num_workers=int(opt.nThreads))\n","\n","    def load_data(self):\n","        return self\n","\n","    def __len__(self):\n","        return min(len(self.dataset), self.opt.max_dataset_size)\n","\n","    def __iter__(self):\n","        for i, data in enumerate(self.dataloader):\n","            if i >= self.opt.max_dataset_size:\n","                break\n","            yield data\n","\n","def CreateDataLoader(opt):\n","    data_loader = CustomDatasetDataLoader()\n","    data_loader.initialize(opt)\n","    return data_loader\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzqVSk4qNlic"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"0YIGMHLxXx4p","executionInfo":{"status":"ok","timestamp":1611509794763,"user_tz":-60,"elapsed":740,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["class Config:\n","  def __init__(self):\n","    self.name = \"Monet\"\n","    self.checkpoints_dir = \"/content/drive/MyDrive/photo2monet/cycleganbayesian/\"\n","    self.model = 'CycleGAN'\n","    self.norm = 'instance'\n","    self.use_dropout = False\n","    self.gpu_ids = '0'\n","    self.which_direction = 'AtoB'\n","\n","    self.batchSize = 1\n","    self.loadSize = 256\n","    self.ratio = 1\n","    self.fineSize = 256\n","    self.input_nc = 3\n","    self.output_nc = 3\n","\n","    self.dataroot_A = '/content/photo_jpg/'\n","    self.dataroot_B = '/content/drive/MyDrive/photo2monet/monetphotos/'\n","    self.resize_or_crop = \"scale_width\"\n","    self.serial_batches = False\n","    self.no_flip = True\n","    self.nThreads = 1\n","    self.max_dataset_size = float(\"inf\")\n","\n","    self.display_winsize = 256\n","    self.display_id = 0\n","    self.display_port = 8097\n","\n","    self.netG_A = 'global'\n","    self.netG_B = 'global'\n","    self.ngf = 32\n","    self.n_downsample_global = 2\n","    self.n_blocks_global = 6\n","\n","    self.netD = 'mult_sacle'\n","    self.num_D_A = 1\n","    self.num_D_B = 1\n","    self.n_layers_D = 3\n","    self.ndf = 64\n","\n","    self.initialized = True\n","    self.isTrain = True\n","\n","    self.display_freq = 100\n","    self.display_single_pane_ncols = 0\n","    self.update_html_freq = 1000\n","    self.print_freq = 100\n","    self.save_latest_freq = 5000\n","    self.save_epoch_freq = 5\n","\n","    self.continue_train = True\n","    self.gamma = 0.1\n","    self.epoch_count = 1\n","    self.phase = \"train\"\n","    self.which_epoch = \"latest\"\n","    self.niter = 50\n","    self.niter_decay = 50\n","    self.beta1 = 0.5\n","    self.lr = 0.0002\n","    self.no_lsgan = False\n","    self.lambda_A = 10.0\n","    self.lambda_B = 10.0\n","    self.lambda_kl = 0.1\n","    self.mc_y = 3\n","    self.mc_x = 3\n","    self.no_html = False\n","    self.lr_policy = 'lambda'\n","    self.lr_decay_iters = 50\n","    self.debug = False\n","    self.need_match = False"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Dikqx1vYuiG","executionInfo":{"status":"ok","timestamp":1611509821260,"user_tz":-60,"elapsed":21608,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"b9598b46-ee31-4e9d-eedf-8e386a4b39d6"},"source":["opt = Config()\n","data_loader = CreateDataLoader(opt)\n","dataset = data_loader.load_data()\n","dataset_size = len(data_loader)\n","print('training images = %d' % dataset_size)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["dataset [UnalignedDataset] was created\n","training images = 7038\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kQsdd3pkmrMq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611509861726,"user_tz":-60,"elapsed":1013,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"f82c58d2-f37e-4c3d-ceb3-d2024992287f"},"source":["# continue train or not\n","if opt.continue_train:\n","    start_epoch = 38\n","    epoch_iter = 0\n","    print('Resuming from epoch %d at iteration %d' % (start_epoch, epoch_iter))\n","else:\n","    start_epoch, epoch_iter = 1, 0"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Resuming from epoch 38 at iteration 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IOHHlShmCWsI","executionInfo":{"status":"ok","timestamp":1611509866700,"user_tz":-60,"elapsed":2542,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["visualizer = Visualizer(opt)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qh7CtkXIedvS","executionInfo":{"status":"ok","timestamp":1611509888151,"user_tz":-60,"elapsed":22308,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"9d09d358-12b1-4491-a3c0-9d02d01a96e7"},"source":["model = CycleGAN()\n","model.initialize(opt)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["cuda is available, we will use gpu!\n","Network initialized!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ZOyuUGjRma9z","executionInfo":{"status":"error","timestamp":1611519805807,"user_tz":-60,"elapsed":3266627,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"dd94d8c9-5ccc-49f6-bb95-851738f83c03"},"source":["# train\n","total_steps = (start_epoch-1) * dataset_size + epoch_iter\n","for epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n","    epoch_start_time = time.time()\n","    if epoch != start_epoch:\n","        epoch_iter = epoch_iter % dataset_size\n","\n","    # for unpaired data\n","    for i, data in enumerate(dataset, start=epoch_iter):\n","        iter_start_time = time.time()\n","        total_steps += opt.batchSize\n","        epoch_iter += opt.batchSize\n","\n","        model.set_input(data)\n","        model.optimize()\n","\n","        if total_steps % 200 == 0:\n","            save_result = total_steps % opt.update_html_freq == 0\n","            visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n","\n","        if total_steps % 100 == 0: \n","            loss = model.get_current_loss()\n","            t = (time.time() - iter_start_time) / opt.batchSize\n","            visualizer.print_current_errors(epoch, epoch_iter, loss, t)\n","            model.update_learning_rate(epoch, epoch_iter, dataset_size)\n","            print('End of step %d / %d \\t Time Taken: %d sec' % (epoch, opt.niter \n","                            + opt.niter_decay, time.time() - epoch_start_time))\n","\n","        #if total_steps % 10000 == 0:\n","            #print('saving model (epoch %d, total_steps %d)' % (epoch, total_steps))\n","            #model.save_model(str(total_steps))\n","\n","          \n","    if epoch % 2 == 0:\n","      print('saving the latest model (epoch %d, total_steps %d)' %\n","                  (epoch, total_steps))\n","      model.save_model('latest')\n","\n","    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n","          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["(epoch: 38, iters: 94, time: 0.905) D_A: 0.086 D_B: 0.012 G_A: 1.453 G_B: 1.248 cyc_G_A: 2.623 cyc_G_B: 1.607 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 182 sec\n","(epoch: 38, iters: 194, time: 8.248) D_A: 0.079 D_B: 0.015 G_A: 1.048 G_B: 1.257 cyc_G_A: 1.479 cyc_G_B: 1.125 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 332 sec\n","(epoch: 38, iters: 294, time: 2.460) D_A: 0.210 D_B: 0.040 G_A: 0.721 G_B: 1.342 cyc_G_A: 1.466 cyc_G_B: 2.193 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 449 sec\n","(epoch: 38, iters: 394, time: 1.059) D_A: 0.039 D_B: 0.016 G_A: 1.232 G_B: 1.370 cyc_G_A: 2.756 cyc_G_B: 2.434 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 540 sec\n","(epoch: 38, iters: 494, time: 0.431) D_A: 0.105 D_B: 0.011 G_A: 0.815 G_B: 1.253 cyc_G_A: 1.660 cyc_G_B: 2.663 kl_EA: 0.000 kl_EB: 0.003 \n","End of step 38 / 100 \t Time Taken: 618 sec\n","(epoch: 38, iters: 594, time: 0.599) D_A: 0.054 D_B: 0.011 G_A: 1.181 G_B: 1.278 cyc_G_A: 1.398 cyc_G_B: 1.210 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 694 sec\n","(epoch: 38, iters: 694, time: 0.962) D_A: 0.082 D_B: 0.012 G_A: 0.815 G_B: 1.271 cyc_G_A: 5.618 cyc_G_B: 3.886 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 755 sec\n","(epoch: 38, iters: 794, time: 0.614) D_A: 0.043 D_B: 0.011 G_A: 1.162 G_B: 1.236 cyc_G_A: 1.677 cyc_G_B: 1.841 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 811 sec\n","(epoch: 38, iters: 894, time: 0.433) D_A: 0.050 D_B: 0.011 G_A: 1.583 G_B: 1.275 cyc_G_A: 1.676 cyc_G_B: 1.633 kl_EA: 0.002 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 865 sec\n","(epoch: 38, iters: 994, time: 1.038) D_A: 0.084 D_B: 0.018 G_A: 0.773 G_B: 1.096 cyc_G_A: 3.934 cyc_G_B: 2.026 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 914 sec\n","(epoch: 38, iters: 1094, time: 0.429) D_A: 0.051 D_B: 0.011 G_A: 1.105 G_B: 1.286 cyc_G_A: 2.128 cyc_G_B: 1.987 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 38 / 100 \t Time Taken: 964 sec\n","(epoch: 38, iters: 1194, time: 0.580) D_A: 0.034 D_B: 0.010 G_A: 1.134 G_B: 1.267 cyc_G_A: 2.096 cyc_G_B: 1.904 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1012 sec\n","(epoch: 38, iters: 1294, time: 0.879) D_A: 0.035 D_B: 0.011 G_A: 1.174 G_B: 1.256 cyc_G_A: 1.418 cyc_G_B: 1.323 kl_EA: 0.001 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 1057 sec\n","(epoch: 38, iters: 1394, time: 0.607) D_A: 0.056 D_B: 0.011 G_A: 0.925 G_B: 1.237 cyc_G_A: 2.342 cyc_G_B: 1.585 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1102 sec\n","(epoch: 38, iters: 1494, time: 0.432) D_A: 0.040 D_B: 0.010 G_A: 1.124 G_B: 1.263 cyc_G_A: 2.766 cyc_G_B: 2.290 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1147 sec\n","(epoch: 38, iters: 1594, time: 0.583) D_A: 0.055 D_B: 0.011 G_A: 1.664 G_B: 1.242 cyc_G_A: 1.639 cyc_G_B: 1.471 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1192 sec\n","(epoch: 38, iters: 1694, time: 0.429) D_A: 0.046 D_B: 0.011 G_A: 1.137 G_B: 1.281 cyc_G_A: 1.924 cyc_G_B: 2.101 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1236 sec\n","(epoch: 38, iters: 1794, time: 0.579) D_A: 0.236 D_B: 0.012 G_A: 0.462 G_B: 1.251 cyc_G_A: 1.463 cyc_G_B: 1.652 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 38 / 100 \t Time Taken: 1282 sec\n","(epoch: 38, iters: 1894, time: 0.435) D_A: 0.036 D_B: 0.040 G_A: 1.464 G_B: 1.492 cyc_G_A: 2.489 cyc_G_B: 1.745 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1325 sec\n","(epoch: 38, iters: 1994, time: 0.581) D_A: 0.042 D_B: 0.011 G_A: 1.298 G_B: 1.243 cyc_G_A: 2.223 cyc_G_B: 2.498 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1369 sec\n","(epoch: 38, iters: 2094, time: 0.445) D_A: 0.043 D_B: 0.010 G_A: 1.084 G_B: 1.215 cyc_G_A: 1.619 cyc_G_B: 1.104 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1412 sec\n","(epoch: 38, iters: 2194, time: 0.579) D_A: 0.084 D_B: 0.010 G_A: 1.727 G_B: 1.279 cyc_G_A: 1.996 cyc_G_B: 2.090 kl_EA: 0.002 kl_EB: 0.002 \n","End of step 38 / 100 \t Time Taken: 1456 sec\n","(epoch: 38, iters: 2294, time: 0.444) D_A: 0.074 D_B: 0.010 G_A: 1.400 G_B: 1.258 cyc_G_A: 6.671 cyc_G_B: 1.199 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1499 sec\n","(epoch: 38, iters: 2394, time: 0.591) D_A: 0.042 D_B: 0.015 G_A: 1.195 G_B: 1.207 cyc_G_A: 1.508 cyc_G_B: 1.361 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1542 sec\n","(epoch: 38, iters: 2494, time: 0.429) D_A: 0.036 D_B: 0.010 G_A: 1.274 G_B: 1.279 cyc_G_A: 2.519 cyc_G_B: 1.841 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1586 sec\n","(epoch: 38, iters: 2594, time: 0.559) D_A: 0.035 D_B: 0.017 G_A: 1.232 G_B: 1.372 cyc_G_A: 3.240 cyc_G_B: 1.926 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1629 sec\n","(epoch: 38, iters: 2694, time: 0.430) D_A: 0.037 D_B: 0.325 G_A: 1.247 G_B: 0.402 cyc_G_A: 2.400 cyc_G_B: 1.457 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 1672 sec\n","(epoch: 38, iters: 2794, time: 0.578) D_A: 0.046 D_B: 0.028 G_A: 1.588 G_B: 1.147 cyc_G_A: 3.309 cyc_G_B: 1.978 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1716 sec\n","(epoch: 38, iters: 2894, time: 0.435) D_A: 0.043 D_B: 0.038 G_A: 1.407 G_B: 1.003 cyc_G_A: 4.120 cyc_G_B: 1.830 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 1759 sec\n","(epoch: 38, iters: 2994, time: 0.559) D_A: 0.041 D_B: 0.014 G_A: 1.075 G_B: 1.260 cyc_G_A: 3.300 cyc_G_B: 2.087 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1802 sec\n","(epoch: 38, iters: 3094, time: 0.428) D_A: 0.083 D_B: 0.013 G_A: 1.050 G_B: 1.283 cyc_G_A: 4.011 cyc_G_B: 2.633 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1846 sec\n","(epoch: 38, iters: 3194, time: 0.566) D_A: 0.035 D_B: 0.011 G_A: 1.266 G_B: 1.269 cyc_G_A: 1.614 cyc_G_B: 2.442 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1889 sec\n","(epoch: 38, iters: 3294, time: 0.426) D_A: 0.090 D_B: 0.012 G_A: 0.853 G_B: 1.173 cyc_G_A: 1.911 cyc_G_B: 2.075 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1932 sec\n","(epoch: 38, iters: 3394, time: 0.591) D_A: 0.060 D_B: 0.016 G_A: 1.468 G_B: 1.395 cyc_G_A: 1.713 cyc_G_B: 2.877 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 1976 sec\n","(epoch: 38, iters: 3494, time: 0.442) D_A: 0.187 D_B: 0.013 G_A: 0.912 G_B: 1.264 cyc_G_A: 3.416 cyc_G_B: 1.724 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2019 sec\n","(epoch: 38, iters: 3594, time: 0.574) D_A: 0.053 D_B: 0.072 G_A: 1.359 G_B: 1.222 cyc_G_A: 1.958 cyc_G_B: 2.036 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2062 sec\n","(epoch: 38, iters: 3694, time: 0.431) D_A: 0.157 D_B: 0.141 G_A: 0.501 G_B: 0.855 cyc_G_A: 1.515 cyc_G_B: 1.976 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2106 sec\n","(epoch: 38, iters: 3794, time: 0.595) D_A: 0.046 D_B: 0.012 G_A: 1.451 G_B: 1.260 cyc_G_A: 3.145 cyc_G_B: 2.277 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 2149 sec\n","(epoch: 38, iters: 3894, time: 0.431) D_A: 0.043 D_B: 0.022 G_A: 1.006 G_B: 1.207 cyc_G_A: 2.528 cyc_G_B: 2.330 kl_EA: 0.000 kl_EB: 0.005 \n","End of step 38 / 100 \t Time Taken: 2193 sec\n","(epoch: 38, iters: 3994, time: 0.557) D_A: 0.046 D_B: 0.096 G_A: 1.198 G_B: 0.953 cyc_G_A: 2.147 cyc_G_B: 2.060 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2236 sec\n","(epoch: 38, iters: 4094, time: 0.428) D_A: 0.045 D_B: 0.039 G_A: 1.071 G_B: 1.034 cyc_G_A: 2.280 cyc_G_B: 1.592 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2279 sec\n","(epoch: 38, iters: 4194, time: 0.572) D_A: 0.069 D_B: 0.013 G_A: 0.828 G_B: 1.280 cyc_G_A: 2.087 cyc_G_B: 2.962 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2323 sec\n","(epoch: 38, iters: 4294, time: 0.429) D_A: 0.032 D_B: 0.012 G_A: 1.196 G_B: 1.261 cyc_G_A: 1.853 cyc_G_B: 1.726 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2366 sec\n","(epoch: 38, iters: 4394, time: 0.560) D_A: 0.052 D_B: 0.017 G_A: 1.415 G_B: 1.245 cyc_G_A: 1.681 cyc_G_B: 1.823 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 2409 sec\n","(epoch: 38, iters: 4494, time: 0.429) D_A: 0.036 D_B: 0.012 G_A: 1.238 G_B: 1.279 cyc_G_A: 1.642 cyc_G_B: 2.014 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2453 sec\n","(epoch: 38, iters: 4594, time: 0.608) D_A: 0.053 D_B: 0.012 G_A: 0.906 G_B: 1.197 cyc_G_A: 2.555 cyc_G_B: 1.322 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2496 sec\n","(epoch: 38, iters: 4694, time: 0.433) D_A: 0.031 D_B: 0.013 G_A: 1.303 G_B: 1.212 cyc_G_A: 2.349 cyc_G_B: 1.925 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 2539 sec\n","(epoch: 38, iters: 4794, time: 0.556) D_A: 0.188 D_B: 0.294 G_A: 0.821 G_B: 1.238 cyc_G_A: 2.871 cyc_G_B: 1.885 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2583 sec\n","(epoch: 38, iters: 4894, time: 0.427) D_A: 0.063 D_B: 0.018 G_A: 1.161 G_B: 1.295 cyc_G_A: 2.444 cyc_G_B: 2.783 kl_EA: 0.000 kl_EB: 0.018 \n","End of step 38 / 100 \t Time Taken: 2626 sec\n","(epoch: 38, iters: 4994, time: 0.575) D_A: 0.041 D_B: 0.023 G_A: 1.053 G_B: 1.151 cyc_G_A: 1.098 cyc_G_B: 1.800 kl_EA: 0.001 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 2670 sec\n","(epoch: 38, iters: 5094, time: 0.431) D_A: 0.047 D_B: 0.095 G_A: 1.172 G_B: 1.220 cyc_G_A: 0.991 cyc_G_B: 2.898 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2713 sec\n","(epoch: 38, iters: 5194, time: 0.569) D_A: 0.062 D_B: 0.019 G_A: 0.831 G_B: 1.229 cyc_G_A: 2.242 cyc_G_B: 2.303 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 2756 sec\n","(epoch: 38, iters: 5294, time: 0.430) D_A: 0.037 D_B: 0.020 G_A: 1.249 G_B: 1.138 cyc_G_A: 2.125 cyc_G_B: 1.806 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 2800 sec\n","(epoch: 38, iters: 5394, time: 0.563) D_A: 0.095 D_B: 0.037 G_A: 1.593 G_B: 0.965 cyc_G_A: 1.470 cyc_G_B: 2.707 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2843 sec\n","(epoch: 38, iters: 5494, time: 0.437) D_A: 0.037 D_B: 0.014 G_A: 1.210 G_B: 1.320 cyc_G_A: 2.052 cyc_G_B: 2.061 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 2886 sec\n","(epoch: 38, iters: 5594, time: 0.563) D_A: 0.157 D_B: 0.083 G_A: 0.530 G_B: 0.757 cyc_G_A: 2.147 cyc_G_B: 2.380 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 38 / 100 \t Time Taken: 2930 sec\n","(epoch: 38, iters: 5694, time: 0.430) D_A: 0.065 D_B: 0.052 G_A: 1.234 G_B: 1.114 cyc_G_A: 2.075 cyc_G_B: 1.636 kl_EA: 0.001 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 2973 sec\n","(epoch: 38, iters: 5794, time: 0.563) D_A: 0.033 D_B: 0.291 G_A: 1.280 G_B: 0.444 cyc_G_A: 2.658 cyc_G_B: 1.789 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 38 / 100 \t Time Taken: 3017 sec\n","(epoch: 38, iters: 5894, time: 0.434) D_A: 0.041 D_B: 0.064 G_A: 1.492 G_B: 0.777 cyc_G_A: 1.268 cyc_G_B: 2.164 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 3060 sec\n","(epoch: 38, iters: 5994, time: 0.562) D_A: 0.050 D_B: 0.017 G_A: 1.092 G_B: 1.269 cyc_G_A: 2.579 cyc_G_B: 2.808 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 3103 sec\n","(epoch: 38, iters: 6094, time: 0.428) D_A: 0.046 D_B: 0.048 G_A: 1.108 G_B: 1.042 cyc_G_A: 2.122 cyc_G_B: 1.587 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 3146 sec\n","(epoch: 38, iters: 6194, time: 0.587) D_A: 0.038 D_B: 0.015 G_A: 1.098 G_B: 1.329 cyc_G_A: 2.166 cyc_G_B: 1.932 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 3190 sec\n","(epoch: 38, iters: 6294, time: 0.432) D_A: 0.033 D_B: 0.030 G_A: 1.297 G_B: 1.219 cyc_G_A: 1.231 cyc_G_B: 1.576 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 3233 sec\n","(epoch: 38, iters: 6394, time: 0.585) D_A: 0.035 D_B: 0.074 G_A: 1.285 G_B: 1.692 cyc_G_A: 1.542 cyc_G_B: 2.551 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 3277 sec\n","(epoch: 38, iters: 6494, time: 0.427) D_A: 0.056 D_B: 0.029 G_A: 0.912 G_B: 0.953 cyc_G_A: 1.609 cyc_G_B: 1.912 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 3320 sec\n","(epoch: 38, iters: 6594, time: 0.570) D_A: 0.078 D_B: 0.025 G_A: 0.832 G_B: 1.302 cyc_G_A: 1.958 cyc_G_B: 2.422 kl_EA: 0.001 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 3363 sec\n","(epoch: 38, iters: 6694, time: 0.435) D_A: 0.038 D_B: 0.015 G_A: 1.274 G_B: 1.340 cyc_G_A: 2.089 cyc_G_B: 2.700 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 3407 sec\n","(epoch: 38, iters: 6794, time: 0.566) D_A: 0.041 D_B: 0.017 G_A: 1.036 G_B: 1.248 cyc_G_A: 2.319 cyc_G_B: 1.814 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 3450 sec\n","(epoch: 38, iters: 6894, time: 0.428) D_A: 0.153 D_B: 0.016 G_A: 0.648 G_B: 1.239 cyc_G_A: 1.671 cyc_G_B: 2.530 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 38 / 100 \t Time Taken: 3493 sec\n","(epoch: 38, iters: 6994, time: 0.573) D_A: 0.043 D_B: 0.017 G_A: 1.328 G_B: 1.319 cyc_G_A: 1.674 cyc_G_B: 2.170 kl_EA: 0.001 kl_EB: 0.001 \n","End of step 38 / 100 \t Time Taken: 3537 sec\n","saving the latest model (epoch 38, total_steps 267444)\n","End of epoch 38 / 100 \t Time Taken: 3556 sec\n","(epoch: 39, iters: 56, time: 0.428) D_A: 0.051 D_B: 0.016 G_A: 1.373 G_B: 1.385 cyc_G_A: 1.561 cyc_G_B: 1.570 kl_EA: 0.003 kl_EB: 0.002 \n","End of step 39 / 100 \t Time Taken: 24 sec\n","(epoch: 39, iters: 156, time: 4.411) D_A: 0.033 D_B: 0.018 G_A: 1.322 G_B: 1.209 cyc_G_A: 2.780 cyc_G_B: 1.510 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 71 sec\n","(epoch: 39, iters: 256, time: 0.437) D_A: 0.044 D_B: 0.013 G_A: 1.102 G_B: 1.269 cyc_G_A: 3.076 cyc_G_B: 1.699 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 115 sec\n","(epoch: 39, iters: 356, time: 0.571) D_A: 0.145 D_B: 0.012 G_A: 1.505 G_B: 1.253 cyc_G_A: 1.803 cyc_G_B: 2.075 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 39 / 100 \t Time Taken: 158 sec\n","(epoch: 39, iters: 456, time: 0.429) D_A: 0.055 D_B: 0.012 G_A: 1.037 G_B: 1.208 cyc_G_A: 1.774 cyc_G_B: 1.833 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 201 sec\n","(epoch: 39, iters: 556, time: 0.566) D_A: 0.046 D_B: 0.014 G_A: 0.993 G_B: 1.242 cyc_G_A: 2.772 cyc_G_B: 1.654 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 244 sec\n","(epoch: 39, iters: 656, time: 0.431) D_A: 0.049 D_B: 0.012 G_A: 0.967 G_B: 1.277 cyc_G_A: 3.517 cyc_G_B: 2.192 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 288 sec\n","(epoch: 39, iters: 756, time: 0.595) D_A: 0.270 D_B: 0.015 G_A: 0.429 G_B: 1.219 cyc_G_A: 1.838 cyc_G_B: 2.415 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 331 sec\n","(epoch: 39, iters: 856, time: 0.446) D_A: 0.065 D_B: 0.013 G_A: 1.720 G_B: 1.258 cyc_G_A: 1.939 cyc_G_B: 1.692 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 374 sec\n","(epoch: 39, iters: 956, time: 0.561) D_A: 0.054 D_B: 0.011 G_A: 0.957 G_B: 1.246 cyc_G_A: 1.587 cyc_G_B: 2.000 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 418 sec\n","(epoch: 39, iters: 1056, time: 0.437) D_A: 0.196 D_B: 0.011 G_A: 1.152 G_B: 1.306 cyc_G_A: 1.379 cyc_G_B: 2.592 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 461 sec\n","(epoch: 39, iters: 1156, time: 0.603) D_A: 0.063 D_B: 0.011 G_A: 1.677 G_B: 1.277 cyc_G_A: 1.880 cyc_G_B: 1.244 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 505 sec\n","(epoch: 39, iters: 1256, time: 0.431) D_A: 0.042 D_B: 0.012 G_A: 1.179 G_B: 1.247 cyc_G_A: 1.925 cyc_G_B: 1.716 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 548 sec\n","(epoch: 39, iters: 1356, time: 0.573) D_A: 0.071 D_B: 0.011 G_A: 0.806 G_B: 1.217 cyc_G_A: 2.010 cyc_G_B: 1.764 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 591 sec\n","(epoch: 39, iters: 1456, time: 0.430) D_A: 0.052 D_B: 0.012 G_A: 1.258 G_B: 1.289 cyc_G_A: 1.694 cyc_G_B: 2.408 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 635 sec\n","(epoch: 39, iters: 1556, time: 0.560) D_A: 0.047 D_B: 0.011 G_A: 1.170 G_B: 1.259 cyc_G_A: 2.831 cyc_G_B: 1.996 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 678 sec\n","(epoch: 39, iters: 1656, time: 0.427) D_A: 0.035 D_B: 0.012 G_A: 1.260 G_B: 1.229 cyc_G_A: 4.091 cyc_G_B: 1.896 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 721 sec\n","(epoch: 39, iters: 1756, time: 0.578) D_A: 0.211 D_B: 0.012 G_A: 1.442 G_B: 1.275 cyc_G_A: 3.597 cyc_G_B: 2.044 kl_EA: 0.002 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 765 sec\n","(epoch: 39, iters: 1856, time: 0.436) D_A: 0.042 D_B: 0.015 G_A: 1.029 G_B: 1.347 cyc_G_A: 1.967 cyc_G_B: 2.201 kl_EA: 0.001 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 808 sec\n","(epoch: 39, iters: 1956, time: 0.578) D_A: 0.047 D_B: 0.012 G_A: 1.147 G_B: 1.247 cyc_G_A: 3.644 cyc_G_B: 1.736 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 851 sec\n","(epoch: 39, iters: 2056, time: 0.428) D_A: 0.047 D_B: 0.013 G_A: 1.395 G_B: 1.248 cyc_G_A: 2.494 cyc_G_B: 2.167 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 895 sec\n","(epoch: 39, iters: 2156, time: 0.574) D_A: 0.039 D_B: 0.011 G_A: 1.154 G_B: 1.285 cyc_G_A: 1.704 cyc_G_B: 2.045 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 938 sec\n","(epoch: 39, iters: 2256, time: 0.428) D_A: 0.039 D_B: 0.013 G_A: 1.348 G_B: 1.266 cyc_G_A: 1.464 cyc_G_B: 1.381 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 981 sec\n","(epoch: 39, iters: 2356, time: 0.561) D_A: 0.091 D_B: 0.012 G_A: 1.352 G_B: 1.219 cyc_G_A: 2.232 cyc_G_B: 2.290 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1025 sec\n","(epoch: 39, iters: 2456, time: 0.429) D_A: 0.036 D_B: 0.014 G_A: 1.206 G_B: 1.232 cyc_G_A: 3.126 cyc_G_B: 2.246 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1068 sec\n","(epoch: 39, iters: 2556, time: 0.565) D_A: 0.034 D_B: 0.012 G_A: 1.221 G_B: 1.282 cyc_G_A: 1.752 cyc_G_B: 2.045 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1111 sec\n","(epoch: 39, iters: 2656, time: 0.428) D_A: 0.048 D_B: 0.012 G_A: 1.538 G_B: 1.192 cyc_G_A: 3.715 cyc_G_B: 2.474 kl_EA: 0.001 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 1155 sec\n","(epoch: 39, iters: 2756, time: 0.570) D_A: 0.045 D_B: 0.012 G_A: 1.367 G_B: 1.275 cyc_G_A: 1.985 cyc_G_B: 2.167 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1198 sec\n","(epoch: 39, iters: 2856, time: 0.429) D_A: 0.057 D_B: 0.013 G_A: 1.659 G_B: 1.265 cyc_G_A: 1.182 cyc_G_B: 2.131 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1241 sec\n","(epoch: 39, iters: 2956, time: 0.560) D_A: 0.048 D_B: 0.012 G_A: 1.363 G_B: 1.237 cyc_G_A: 1.876 cyc_G_B: 2.136 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1285 sec\n","(epoch: 39, iters: 3056, time: 0.425) D_A: 0.035 D_B: 0.011 G_A: 1.243 G_B: 1.298 cyc_G_A: 1.549 cyc_G_B: 1.939 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1328 sec\n","(epoch: 39, iters: 3156, time: 0.578) D_A: 0.047 D_B: 0.011 G_A: 1.001 G_B: 1.280 cyc_G_A: 2.473 cyc_G_B: 1.756 kl_EA: 0.002 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 1371 sec\n","(epoch: 39, iters: 3256, time: 0.430) D_A: 0.055 D_B: 0.011 G_A: 1.317 G_B: 1.251 cyc_G_A: 1.508 cyc_G_B: 2.266 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 39 / 100 \t Time Taken: 1415 sec\n","(epoch: 39, iters: 3356, time: 0.576) D_A: 0.046 D_B: 0.012 G_A: 1.471 G_B: 1.243 cyc_G_A: 2.392 cyc_G_B: 2.312 kl_EA: 0.000 kl_EB: 0.003 \n","End of step 39 / 100 \t Time Taken: 1458 sec\n","(epoch: 39, iters: 3456, time: 0.427) D_A: 0.061 D_B: 0.012 G_A: 1.029 G_B: 1.234 cyc_G_A: 1.657 cyc_G_B: 1.937 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1501 sec\n","(epoch: 39, iters: 3556, time: 0.596) D_A: 0.062 D_B: 0.012 G_A: 0.909 G_B: 1.215 cyc_G_A: 2.001 cyc_G_B: 1.979 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 1545 sec\n","(epoch: 39, iters: 3656, time: 0.428) D_A: 0.033 D_B: 0.011 G_A: 1.332 G_B: 1.247 cyc_G_A: 1.726 cyc_G_B: 1.239 kl_EA: 0.007 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1588 sec\n","(epoch: 39, iters: 3756, time: 0.587) D_A: 0.061 D_B: 0.011 G_A: 1.519 G_B: 1.219 cyc_G_A: 2.721 cyc_G_B: 1.261 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1632 sec\n","(epoch: 39, iters: 3856, time: 0.432) D_A: 0.068 D_B: 0.011 G_A: 1.440 G_B: 1.285 cyc_G_A: 1.296 cyc_G_B: 1.526 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 39 / 100 \t Time Taken: 1675 sec\n","(epoch: 39, iters: 3956, time: 0.572) D_A: 0.041 D_B: 0.011 G_A: 1.100 G_B: 1.286 cyc_G_A: 1.917 cyc_G_B: 1.477 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 1719 sec\n","(epoch: 39, iters: 4056, time: 0.429) D_A: 0.042 D_B: 0.012 G_A: 1.192 G_B: 1.351 cyc_G_A: 2.834 cyc_G_B: 2.480 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 1762 sec\n","(epoch: 39, iters: 4156, time: 0.577) D_A: 0.038 D_B: 0.012 G_A: 1.388 G_B: 1.314 cyc_G_A: 0.952 cyc_G_B: 2.792 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1805 sec\n","(epoch: 39, iters: 4256, time: 0.428) D_A: 0.118 D_B: 0.019 G_A: 0.634 G_B: 1.176 cyc_G_A: 1.495 cyc_G_B: 1.938 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 1849 sec\n","(epoch: 39, iters: 4356, time: 0.566) D_A: 0.036 D_B: 0.015 G_A: 1.216 G_B: 1.238 cyc_G_A: 2.505 cyc_G_B: 1.907 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 39 / 100 \t Time Taken: 1892 sec\n","(epoch: 39, iters: 4456, time: 0.431) D_A: 0.053 D_B: 0.098 G_A: 1.441 G_B: 0.787 cyc_G_A: 2.696 cyc_G_B: 2.533 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 1936 sec\n","(epoch: 39, iters: 4556, time: 0.580) D_A: 0.059 D_B: 0.040 G_A: 1.011 G_B: 1.053 cyc_G_A: 2.391 cyc_G_B: 2.734 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 1979 sec\n","(epoch: 39, iters: 4656, time: 0.429) D_A: 0.232 D_B: 0.376 G_A: 1.555 G_B: 1.416 cyc_G_A: 0.850 cyc_G_B: 1.560 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 2022 sec\n","(epoch: 39, iters: 4756, time: 0.585) D_A: 0.070 D_B: 0.026 G_A: 0.912 G_B: 1.238 cyc_G_A: 2.024 cyc_G_B: 2.179 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2066 sec\n","(epoch: 39, iters: 4856, time: 0.447) D_A: 0.037 D_B: 0.022 G_A: 1.356 G_B: 1.289 cyc_G_A: 2.932 cyc_G_B: 1.207 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 2109 sec\n","(epoch: 39, iters: 4956, time: 0.603) D_A: 0.034 D_B: 0.015 G_A: 1.210 G_B: 1.274 cyc_G_A: 1.449 cyc_G_B: 1.890 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2152 sec\n","(epoch: 39, iters: 5056, time: 0.429) D_A: 0.044 D_B: 0.026 G_A: 1.039 G_B: 1.026 cyc_G_A: 0.987 cyc_G_B: 1.813 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2196 sec\n","(epoch: 39, iters: 5156, time: 0.579) D_A: 0.035 D_B: 0.017 G_A: 1.290 G_B: 1.207 cyc_G_A: 2.349 cyc_G_B: 1.649 kl_EA: 0.000 kl_EB: 0.004 \n","End of step 39 / 100 \t Time Taken: 2239 sec\n","(epoch: 39, iters: 5256, time: 0.429) D_A: 0.035 D_B: 0.026 G_A: 1.362 G_B: 1.263 cyc_G_A: 2.391 cyc_G_B: 1.941 kl_EA: 0.001 kl_EB: 0.004 \n","End of step 39 / 100 \t Time Taken: 2282 sec\n","(epoch: 39, iters: 5356, time: 0.568) D_A: 0.046 D_B: 0.018 G_A: 1.159 G_B: 1.298 cyc_G_A: 2.438 cyc_G_B: 2.123 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 2326 sec\n","(epoch: 39, iters: 5456, time: 0.428) D_A: 0.035 D_B: 0.042 G_A: 1.378 G_B: 0.876 cyc_G_A: 3.042 cyc_G_B: 1.253 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2369 sec\n","(epoch: 39, iters: 5556, time: 0.579) D_A: 0.063 D_B: 0.014 G_A: 1.354 G_B: 1.289 cyc_G_A: 2.226 cyc_G_B: 2.130 kl_EA: 0.005 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2412 sec\n","(epoch: 39, iters: 5656, time: 0.434) D_A: 0.054 D_B: 0.013 G_A: 1.405 G_B: 1.340 cyc_G_A: 1.038 cyc_G_B: 3.284 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2455 sec\n","(epoch: 39, iters: 5756, time: 0.574) D_A: 0.055 D_B: 0.014 G_A: 0.967 G_B: 1.213 cyc_G_A: 2.073 cyc_G_B: 2.870 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2499 sec\n","(epoch: 39, iters: 5856, time: 0.427) D_A: 0.039 D_B: 0.014 G_A: 1.169 G_B: 1.203 cyc_G_A: 2.825 cyc_G_B: 1.942 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2542 sec\n","(epoch: 39, iters: 5956, time: 0.577) D_A: 0.040 D_B: 0.013 G_A: 1.070 G_B: 1.351 cyc_G_A: 2.077 cyc_G_B: 1.945 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 2585 sec\n","(epoch: 39, iters: 6056, time: 0.427) D_A: 0.144 D_B: 0.013 G_A: 1.349 G_B: 1.314 cyc_G_A: 2.113 cyc_G_B: 2.192 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2628 sec\n","(epoch: 39, iters: 6156, time: 0.569) D_A: 0.062 D_B: 0.013 G_A: 1.569 G_B: 1.345 cyc_G_A: 2.514 cyc_G_B: 2.493 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2672 sec\n","(epoch: 39, iters: 6256, time: 0.431) D_A: 0.037 D_B: 0.012 G_A: 1.427 G_B: 1.265 cyc_G_A: 1.242 cyc_G_B: 1.795 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2715 sec\n","(epoch: 39, iters: 6356, time: 0.576) D_A: 0.065 D_B: 0.012 G_A: 1.290 G_B: 1.194 cyc_G_A: 2.060 cyc_G_B: 1.260 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2758 sec\n","(epoch: 39, iters: 6456, time: 0.426) D_A: 0.048 D_B: 0.012 G_A: 0.999 G_B: 1.254 cyc_G_A: 2.238 cyc_G_B: 1.136 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2801 sec\n","(epoch: 39, iters: 6556, time: 0.597) D_A: 0.039 D_B: 0.017 G_A: 1.343 G_B: 1.085 cyc_G_A: 1.562 cyc_G_B: 1.198 kl_EA: 0.000 kl_EB: 0.003 \n","End of step 39 / 100 \t Time Taken: 2845 sec\n","(epoch: 39, iters: 6656, time: 0.426) D_A: 0.109 D_B: 0.013 G_A: 0.852 G_B: 1.231 cyc_G_A: 2.293 cyc_G_B: 1.508 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 2888 sec\n","(epoch: 39, iters: 6756, time: 0.572) D_A: 0.106 D_B: 0.013 G_A: 1.045 G_B: 1.243 cyc_G_A: 2.327 cyc_G_B: 1.713 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 2931 sec\n","(epoch: 39, iters: 6856, time: 0.426) D_A: 0.043 D_B: 0.011 G_A: 1.351 G_B: 1.287 cyc_G_A: 1.858 cyc_G_B: 1.724 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 39 / 100 \t Time Taken: 2974 sec\n","(epoch: 39, iters: 6956, time: 0.567) D_A: 0.054 D_B: 0.017 G_A: 1.017 G_B: 1.241 cyc_G_A: 2.670 cyc_G_B: 2.124 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 39 / 100 \t Time Taken: 3018 sec\n","End of epoch 39 / 100 \t Time Taken: 3053 sec\n","(epoch: 40, iters: 18, time: 0.428) D_A: 0.041 D_B: 0.126 G_A: 1.308 G_B: 1.072 cyc_G_A: 1.737 cyc_G_B: 3.276 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 7 sec\n","(epoch: 40, iters: 118, time: 0.560) D_A: 0.265 D_B: 0.012 G_A: 0.594 G_B: 1.173 cyc_G_A: 2.758 cyc_G_B: 2.724 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 51 sec\n","(epoch: 40, iters: 218, time: 0.433) D_A: 0.057 D_B: 0.013 G_A: 1.045 G_B: 1.279 cyc_G_A: 2.243 cyc_G_B: 1.607 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 94 sec\n","(epoch: 40, iters: 318, time: 0.549) D_A: 0.095 D_B: 0.013 G_A: 1.394 G_B: 1.340 cyc_G_A: 3.479 cyc_G_B: 3.244 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 137 sec\n","(epoch: 40, iters: 418, time: 0.427) D_A: 0.048 D_B: 0.173 G_A: 1.173 G_B: 0.542 cyc_G_A: 1.353 cyc_G_B: 1.535 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 180 sec\n","(epoch: 40, iters: 518, time: 0.605) D_A: 0.079 D_B: 0.055 G_A: 1.368 G_B: 1.018 cyc_G_A: 2.224 cyc_G_B: 2.289 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 224 sec\n","(epoch: 40, iters: 618, time: 0.427) D_A: 0.056 D_B: 0.192 G_A: 1.000 G_B: 0.454 cyc_G_A: 1.065 cyc_G_B: 1.625 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 267 sec\n","(epoch: 40, iters: 718, time: 0.566) D_A: 0.050 D_B: 0.034 G_A: 1.100 G_B: 1.426 cyc_G_A: 4.702 cyc_G_B: 2.073 kl_EA: 0.003 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 311 sec\n","(epoch: 40, iters: 818, time: 0.429) D_A: 0.092 D_B: 0.016 G_A: 0.759 G_B: 1.224 cyc_G_A: 4.701 cyc_G_B: 1.892 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 354 sec\n","(epoch: 40, iters: 918, time: 0.567) D_A: 0.042 D_B: 0.016 G_A: 1.412 G_B: 1.356 cyc_G_A: 3.480 cyc_G_B: 1.605 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 397 sec\n","(epoch: 40, iters: 1018, time: 0.428) D_A: 0.064 D_B: 0.013 G_A: 0.859 G_B: 1.250 cyc_G_A: 2.655 cyc_G_B: 2.060 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 440 sec\n","(epoch: 40, iters: 1118, time: 0.587) D_A: 0.033 D_B: 0.013 G_A: 1.272 G_B: 1.191 cyc_G_A: 2.650 cyc_G_B: 1.979 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 484 sec\n","(epoch: 40, iters: 1218, time: 0.430) D_A: 0.035 D_B: 0.015 G_A: 1.243 G_B: 1.292 cyc_G_A: 3.787 cyc_G_B: 2.533 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 40 / 100 \t Time Taken: 527 sec\n","(epoch: 40, iters: 1318, time: 0.587) D_A: 0.053 D_B: 0.013 G_A: 1.181 G_B: 1.290 cyc_G_A: 1.830 cyc_G_B: 1.603 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 570 sec\n","(epoch: 40, iters: 1418, time: 0.427) D_A: 0.135 D_B: 0.016 G_A: 0.835 G_B: 1.161 cyc_G_A: 3.115 cyc_G_B: 2.131 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 613 sec\n","(epoch: 40, iters: 1518, time: 0.611) D_A: 0.042 D_B: 0.014 G_A: 1.211 G_B: 1.337 cyc_G_A: 2.205 cyc_G_B: 1.687 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 657 sec\n","(epoch: 40, iters: 1618, time: 0.428) D_A: 0.055 D_B: 0.127 G_A: 1.192 G_B: 0.899 cyc_G_A: 2.661 cyc_G_B: 1.647 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 700 sec\n","(epoch: 40, iters: 1718, time: 0.571) D_A: 0.051 D_B: 0.012 G_A: 0.955 G_B: 1.260 cyc_G_A: 2.006 cyc_G_B: 1.891 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 743 sec\n","(epoch: 40, iters: 1818, time: 0.441) D_A: 0.032 D_B: 0.013 G_A: 1.259 G_B: 1.325 cyc_G_A: 2.982 cyc_G_B: 1.170 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 787 sec\n","(epoch: 40, iters: 1918, time: 0.575) D_A: 0.371 D_B: 0.012 G_A: 0.541 G_B: 1.189 cyc_G_A: 2.053 cyc_G_B: 2.039 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 830 sec\n","(epoch: 40, iters: 2018, time: 0.432) D_A: 0.092 D_B: 0.012 G_A: 1.640 G_B: 1.296 cyc_G_A: 3.547 cyc_G_B: 2.185 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 873 sec\n","(epoch: 40, iters: 2118, time: 0.579) D_A: 0.124 D_B: 0.012 G_A: 1.261 G_B: 1.268 cyc_G_A: 3.231 cyc_G_B: 2.439 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 917 sec\n","(epoch: 40, iters: 2218, time: 0.447) D_A: 0.044 D_B: 0.012 G_A: 1.357 G_B: 1.257 cyc_G_A: 3.499 cyc_G_B: 1.490 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 960 sec\n","(epoch: 40, iters: 2318, time: 0.556) D_A: 0.039 D_B: 0.012 G_A: 1.073 G_B: 1.271 cyc_G_A: 2.647 cyc_G_B: 2.703 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 1003 sec\n","(epoch: 40, iters: 2418, time: 0.431) D_A: 0.086 D_B: 0.012 G_A: 0.797 G_B: 1.259 cyc_G_A: 2.177 cyc_G_B: 1.620 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1047 sec\n","(epoch: 40, iters: 2518, time: 0.580) D_A: 0.075 D_B: 0.012 G_A: 1.028 G_B: 1.230 cyc_G_A: 2.328 cyc_G_B: 1.243 kl_EA: 0.001 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 1090 sec\n","(epoch: 40, iters: 2618, time: 0.431) D_A: 0.072 D_B: 0.012 G_A: 0.945 G_B: 1.287 cyc_G_A: 2.079 cyc_G_B: 2.141 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1133 sec\n","(epoch: 40, iters: 2718, time: 0.582) D_A: 0.061 D_B: 0.011 G_A: 1.414 G_B: 1.217 cyc_G_A: 1.256 cyc_G_B: 2.479 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1176 sec\n","(epoch: 40, iters: 2818, time: 0.430) D_A: 0.113 D_B: 0.011 G_A: 0.874 G_B: 1.220 cyc_G_A: 1.607 cyc_G_B: 1.616 kl_EA: 0.001 kl_EB: 0.002 \n","End of step 40 / 100 \t Time Taken: 1220 sec\n","(epoch: 40, iters: 2918, time: 0.578) D_A: 0.058 D_B: 0.011 G_A: 0.890 G_B: 1.264 cyc_G_A: 1.255 cyc_G_B: 1.659 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1263 sec\n","(epoch: 40, iters: 3018, time: 0.428) D_A: 0.073 D_B: 0.013 G_A: 1.265 G_B: 1.284 cyc_G_A: 2.828 cyc_G_B: 1.730 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1306 sec\n","(epoch: 40, iters: 3118, time: 0.567) D_A: 0.077 D_B: 0.011 G_A: 0.947 G_B: 1.247 cyc_G_A: 2.397 cyc_G_B: 2.907 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1350 sec\n","(epoch: 40, iters: 3218, time: 0.437) D_A: 0.102 D_B: 0.011 G_A: 0.690 G_B: 1.274 cyc_G_A: 2.130 cyc_G_B: 2.178 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 1393 sec\n","(epoch: 40, iters: 3318, time: 0.588) D_A: 0.060 D_B: 0.012 G_A: 1.104 G_B: 1.257 cyc_G_A: 2.444 cyc_G_B: 2.432 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1436 sec\n","(epoch: 40, iters: 3418, time: 0.429) D_A: 0.082 D_B: 0.012 G_A: 1.282 G_B: 1.224 cyc_G_A: 2.100 cyc_G_B: 2.399 kl_EA: 0.000 kl_EB: 0.006 \n","End of step 40 / 100 \t Time Taken: 1480 sec\n","(epoch: 40, iters: 3518, time: 0.573) D_A: 0.171 D_B: 0.011 G_A: 1.332 G_B: 1.331 cyc_G_A: 1.106 cyc_G_B: 1.941 kl_EA: 0.002 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1523 sec\n","(epoch: 40, iters: 3618, time: 0.430) D_A: 0.046 D_B: 0.012 G_A: 1.365 G_B: 1.278 cyc_G_A: 2.727 cyc_G_B: 1.783 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 1566 sec\n","(epoch: 40, iters: 3718, time: 0.566) D_A: 0.034 D_B: 0.014 G_A: 1.228 G_B: 1.338 cyc_G_A: 1.797 cyc_G_B: 1.821 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1610 sec\n","(epoch: 40, iters: 3818, time: 0.440) D_A: 0.039 D_B: 0.012 G_A: 1.507 G_B: 1.223 cyc_G_A: 2.225 cyc_G_B: 1.732 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1653 sec\n","(epoch: 40, iters: 3918, time: 0.579) D_A: 0.087 D_B: 0.015 G_A: 0.978 G_B: 1.302 cyc_G_A: 2.033 cyc_G_B: 2.156 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 1697 sec\n","(epoch: 40, iters: 4018, time: 0.433) D_A: 0.036 D_B: 0.012 G_A: 1.273 G_B: 1.259 cyc_G_A: 1.928 cyc_G_B: 2.260 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1740 sec\n","(epoch: 40, iters: 4118, time: 0.567) D_A: 0.040 D_B: 0.023 G_A: 1.069 G_B: 1.401 cyc_G_A: 2.321 cyc_G_B: 2.363 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1783 sec\n","(epoch: 40, iters: 4218, time: 0.427) D_A: 0.035 D_B: 0.012 G_A: 1.252 G_B: 1.238 cyc_G_A: 3.077 cyc_G_B: 1.955 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1827 sec\n","(epoch: 40, iters: 4318, time: 0.567) D_A: 0.035 D_B: 0.013 G_A: 1.140 G_B: 1.313 cyc_G_A: 1.206 cyc_G_B: 1.986 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1870 sec\n","(epoch: 40, iters: 4418, time: 0.429) D_A: 0.032 D_B: 0.011 G_A: 1.254 G_B: 1.264 cyc_G_A: 2.936 cyc_G_B: 1.734 kl_EA: 0.000 kl_EB: 0.017 \n","End of step 40 / 100 \t Time Taken: 1913 sec\n","(epoch: 40, iters: 4518, time: 0.565) D_A: 0.038 D_B: 0.010 G_A: 1.253 G_B: 1.285 cyc_G_A: 2.176 cyc_G_B: 1.555 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 1957 sec\n","(epoch: 40, iters: 4618, time: 0.432) D_A: 0.316 D_B: 0.012 G_A: 0.381 G_B: 1.162 cyc_G_A: 4.660 cyc_G_B: 1.395 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2000 sec\n","(epoch: 40, iters: 4718, time: 0.574) D_A: 0.154 D_B: 0.012 G_A: 1.102 G_B: 1.280 cyc_G_A: 1.670 cyc_G_B: 2.187 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2043 sec\n","(epoch: 40, iters: 4818, time: 0.433) D_A: 0.076 D_B: 0.014 G_A: 0.918 G_B: 1.177 cyc_G_A: 2.134 cyc_G_B: 1.738 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2087 sec\n","(epoch: 40, iters: 4918, time: 0.563) D_A: 0.082 D_B: 0.014 G_A: 0.928 G_B: 1.301 cyc_G_A: 2.061 cyc_G_B: 2.346 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2130 sec\n","(epoch: 40, iters: 5018, time: 0.428) D_A: 0.076 D_B: 0.049 G_A: 1.485 G_B: 1.578 cyc_G_A: 2.577 cyc_G_B: 1.533 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2174 sec\n","(epoch: 40, iters: 5118, time: 0.566) D_A: 0.047 D_B: 0.014 G_A: 1.264 G_B: 1.310 cyc_G_A: 2.492 cyc_G_B: 1.605 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2217 sec\n","(epoch: 40, iters: 5218, time: 0.428) D_A: 0.034 D_B: 0.443 G_A: 1.208 G_B: 0.497 cyc_G_A: 3.122 cyc_G_B: 2.182 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2260 sec\n","(epoch: 40, iters: 5318, time: 0.603) D_A: 0.038 D_B: 0.408 G_A: 1.035 G_B: 0.335 cyc_G_A: 1.564 cyc_G_B: 1.515 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2304 sec\n","(epoch: 40, iters: 5418, time: 0.428) D_A: 0.037 D_B: 0.433 G_A: 1.198 G_B: 0.315 cyc_G_A: 3.127 cyc_G_B: 1.717 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 2347 sec\n","(epoch: 40, iters: 5518, time: 0.572) D_A: 0.151 D_B: 0.142 G_A: 1.099 G_B: 0.745 cyc_G_A: 2.328 cyc_G_B: 2.227 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2390 sec\n","(epoch: 40, iters: 5618, time: 0.433) D_A: 0.035 D_B: 0.041 G_A: 1.265 G_B: 1.054 cyc_G_A: 2.222 cyc_G_B: 2.388 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2433 sec\n","(epoch: 40, iters: 5718, time: 0.577) D_A: 0.042 D_B: 0.028 G_A: 1.202 G_B: 1.341 cyc_G_A: 2.036 cyc_G_B: 1.550 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2477 sec\n","(epoch: 40, iters: 5818, time: 0.431) D_A: 0.059 D_B: 0.022 G_A: 1.135 G_B: 1.257 cyc_G_A: 2.191 cyc_G_B: 1.735 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2520 sec\n","(epoch: 40, iters: 5918, time: 0.606) D_A: 0.035 D_B: 0.018 G_A: 1.383 G_B: 1.196 cyc_G_A: 1.487 cyc_G_B: 1.382 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2563 sec\n","(epoch: 40, iters: 6018, time: 0.444) D_A: 0.070 D_B: 0.016 G_A: 1.222 G_B: 1.345 cyc_G_A: 1.755 cyc_G_B: 1.615 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2607 sec\n","(epoch: 40, iters: 6118, time: 0.563) D_A: 0.221 D_B: 0.016 G_A: 0.840 G_B: 1.450 cyc_G_A: 1.770 cyc_G_B: 3.453 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 2650 sec\n","(epoch: 40, iters: 6218, time: 0.426) D_A: 0.035 D_B: 0.030 G_A: 1.213 G_B: 0.967 cyc_G_A: 2.382 cyc_G_B: 2.717 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2693 sec\n","(epoch: 40, iters: 6318, time: 0.576) D_A: 0.038 D_B: 0.026 G_A: 1.302 G_B: 1.212 cyc_G_A: 1.110 cyc_G_B: 1.761 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 2737 sec\n","(epoch: 40, iters: 6418, time: 0.434) D_A: 0.285 D_B: 0.014 G_A: 1.552 G_B: 1.318 cyc_G_A: 3.008 cyc_G_B: 2.906 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2780 sec\n","(epoch: 40, iters: 6518, time: 0.589) D_A: 0.081 D_B: 0.013 G_A: 0.937 G_B: 1.350 cyc_G_A: 1.445 cyc_G_B: 2.279 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2823 sec\n","(epoch: 40, iters: 6618, time: 0.426) D_A: 0.039 D_B: 0.013 G_A: 1.400 G_B: 1.297 cyc_G_A: 1.223 cyc_G_B: 1.877 kl_EA: 0.000 kl_EB: 0.001 \n","End of step 40 / 100 \t Time Taken: 2867 sec\n","(epoch: 40, iters: 6718, time: 0.610) D_A: 0.038 D_B: 0.013 G_A: 1.067 G_B: 1.270 cyc_G_A: 2.108 cyc_G_B: 2.347 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2910 sec\n","(epoch: 40, iters: 6818, time: 0.428) D_A: 0.051 D_B: 0.012 G_A: 1.090 G_B: 1.219 cyc_G_A: 1.548 cyc_G_B: 1.950 kl_EA: 0.000 kl_EB: 0.002 \n","End of step 40 / 100 \t Time Taken: 2953 sec\n","(epoch: 40, iters: 6918, time: 0.588) D_A: 0.076 D_B: 0.014 G_A: 0.781 G_B: 1.246 cyc_G_A: 1.749 cyc_G_B: 1.786 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 2997 sec\n","(epoch: 40, iters: 7018, time: 0.427) D_A: 0.046 D_B: 0.013 G_A: 1.042 G_B: 1.235 cyc_G_A: 2.101 cyc_G_B: 2.875 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 40 / 100 \t Time Taken: 3040 sec\n","saving the latest model (epoch 40, total_steps 281520)\n","End of epoch 40 / 100 \t Time Taken: 3049 sec\n","(epoch: 41, iters: 80, time: 0.566) D_A: 0.134 D_B: 0.014 G_A: 0.771 G_B: 1.217 cyc_G_A: 2.363 cyc_G_B: 2.106 kl_EA: 0.001 kl_EB: 0.000 \n","End of step 41 / 100 \t Time Taken: 34 sec\n","(epoch: 41, iters: 180, time: 0.426) D_A: 0.043 D_B: 0.012 G_A: 1.155 G_B: 1.284 cyc_G_A: 1.236 cyc_G_B: 1.875 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 41 / 100 \t Time Taken: 77 sec\n","(epoch: 41, iters: 280, time: 0.586) D_A: 0.176 D_B: 0.648 G_A: 1.615 G_B: 1.322 cyc_G_A: 1.830 cyc_G_B: 2.659 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 41 / 100 \t Time Taken: 121 sec\n","(epoch: 41, iters: 380, time: 0.427) D_A: 0.044 D_B: 0.026 G_A: 1.060 G_B: 1.048 cyc_G_A: 3.672 cyc_G_B: 1.836 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 41 / 100 \t Time Taken: 164 sec\n","(epoch: 41, iters: 480, time: 0.584) D_A: 0.043 D_B: 0.017 G_A: 1.355 G_B: 1.349 cyc_G_A: 1.484 cyc_G_B: 3.052 kl_EA: 0.000 kl_EB: 0.000 \n","End of step 41 / 100 \t Time Taken: 207 sec\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-7387b71ca594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-b08d0e7b1a72>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_E_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-b08d0e7b1a72>\u001b[0m in \u001b[0;36mbackward_G\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mfake_A_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_A_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mrec_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetG_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_A_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mloss_cycle_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmc_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-edd4dc5bef30>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMultiscaleDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-edd4dc5bef30>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}