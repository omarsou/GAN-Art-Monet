{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BayesianCycleGan_predict.ipynb","provenance":[],"collapsed_sections":["W14SklNDQRp3","5ssmf_-wEP-L","6pZCuvQFDVjo","f7KroGBaK9r5"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1lTlKV8aqHJgApzKN-iFCxWcwCOTG2aYI","authorship_tag":"ABX9TyPD1REzbowapyoIApf74Az5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lQY5GnuVu57G"},"source":["Based on the paper : https://ieeexplore.ieee.org/document/9186319\n","\n","Based on the code : https://github.com/ranery/Bayesian-CycleGAN\n","\n"]},{"cell_type":"code","metadata":{"id":"rWLRypLUdymp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611520441085,"user_tz":-60,"elapsed":3799,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"d53c01b1-4301-4d6d-f233-25c8dc5bc482"},"source":["!nvidia-smi -L\n","!pip install --upgrade --force-reinstall --no-deps kaggle"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-c74f44ec-5471-5c48-a87c-040f983dbcc0)\n","Collecting kaggle\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/33/365c0d13f07a2a54744d027fe20b60dacdfdfb33bc04746db6ad0b79340b/kaggle-1.5.10.tar.gz (59kB)\n","\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.10-cp36-none-any.whl size=73269 sha256=ea6176d83c165d643950dc9812526389ce3f6065266d65b493e03996c03d11cc\n","  Stored in directory: /root/.cache/pip/wheels/3a/d1/7e/6ce09b72b770149802c653a02783821629146983ee5a360f10\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Found existing installation: kaggle 1.5.10\n","    Uninstalling kaggle-1.5.10:\n","      Successfully uninstalled kaggle-1.5.10\n","Successfully installed kaggle-1.5.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RVBG-hkSD4po"},"source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","from torch.nn.parameter import Parameter\n","import torch.utils.data as data\n","\n","import functools\n","import numpy as np\n","import cv2\n","from scipy import misc\n","import time, itertools\n","import random\n","from collections import OrderedDict\n","\n","from PIL import Image\n","\n","import os\n","import os.path"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5jk3g9Jd54L"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"zbg0j_NSd7Uh","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1611520479351,"user_tz":-60,"elapsed":13315,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"e923d83c-e827-4e4a-e21f-6abe6dc9ef19"},"source":["from google.colab import files\n","# Here you should upload your Kaggle API key (see : https://www.kaggle.com/docs/api (Authentification paragraph))\n","files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-0fb333e7-28b6-464a-b7dd-d3ced6afc44d\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-0fb333e7-28b6-464a-b7dd-d3ced6afc44d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving kaggle.json to kaggle.json\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"negalov\",\"key\":\"f2d706af1447ced98029686098226cba\"}'}"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"AxCtfHOneAH2"},"source":["! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","! kaggle datasets list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gt93ENw1eAxB"},"source":["! kaggle competitions download -c gan-getting-started"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_L190GHeE16"},"source":["! unzip /content/gan-getting-started.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W14SklNDQRp3"},"source":["# Utils"]},{"cell_type":"code","metadata":{"id":"jjkvBxlIPDUK"},"source":["class ImagePool():\n","    def __init__(self, pool_size):\n","        self.pool_size = pool_size\n","        if self.pool_size > 0:\n","            self.num_imgs = 0\n","            self.images = []\n","\n","    def query(self, images):\n","        if self.pool_size == 0:\n","            return Variable(images)\n","        return_images = []\n","        for image in images:\n","            image = torch.unsqueeze(image, 0)\n","            if self.num_imgs < self.pool_size:\n","                self.num_imgs = self.num_imgs + 1\n","                self.images.append(image)\n","                return_images.append(image)\n","            else:\n","                p = random.uniform(0, 1)\n","                if p > 0.5:\n","                    random_id = random.randint(0, self.pool_size-1)\n","                    tmp = self.images[random_id].clone()\n","                    self.images[random_id] = image\n","                    return_images.append(tmp)\n","                else:\n","                    return_images.append(image)\n","        return_images = Variable(torch.cat(return_images, 0))\n","        return return_images\n","\n","def tensor2im(image_tensor, imtype=np.uint8):\n","    image_numpy = image_tensor.detach().cpu().float().numpy()\n","    #if image_numpy.shape[0] == 1:\n","    image_numpy = image_numpy[0]\n","    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n","    return image_numpy.astype(imtype)\n","\n","\n","def diagnose_network(net, name='network'):\n","    mean = 0.0\n","    count = 0\n","    for param in net.parameters():\n","        if param.grad is not None:\n","            mean += torch.mean(torch.abs(param.grad.data))\n","            count += 1\n","    if count > 0:\n","        mean = mean / count\n","    print(name)\n","    print(mean)\n","\n","\n","def save_image(image_numpy, image_path):\n","    image_pil = Image.fromarray(image_numpy)\n","    image_pil.save(image_path)\n","\n","\n","def print_numpy(x, val=True, shp=False):\n","    x = x.astype(np.float64)\n","    if shp:\n","        print('shape,', x.shape)\n","    if val:\n","        x = x.flatten()\n","        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n","            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n","\n","\n","def mkdirs(paths):\n","    if isinstance(paths, list) and not isinstance(paths, str):\n","        for path in paths:\n","            mkdir(path)\n","    else:\n","        mkdir(paths)\n","\n","\n","def mkdir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","class Visualizer():\n","    def __init__(self, opt):\n","        # self.opt = opt\n","        self.display_id = opt.display_id\n","        self.use_html = opt.isTrain and not opt.no_html\n","        self.win_size = opt.display_winsize\n","        self.name = opt.name\n","        self.opt = opt\n","        self.saved = False\n","        self.img_dir = os.path.join(opt.checkpoints_dir, opt.name, 'images')\n","        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, 'loss_log.txt')\n","        try:\n","          with open(self.log_name, \"a\") as log_file:\n","            now = time.strftime(\"%c\")\n","            log_file.write('================ Training Loss (%s) ================\\n' % now)\n","        except FileNotFoundError:\n","          with open(self.log_name, \"w\") as log_file:\n","            now = time.strftime(\"%c\")\n","            log_file.write('================ Training Loss (%s) ================\\n' % now)\n","\n","    def reset(self):\n","        self.saved = False\n","\n","    # |visuals|: dictionary of images to display or save\n","    def display_current_results(self, visuals, epoch, save_result):\n","        for label, image_numpy in visuals.items():\n","            img_path = os.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))\n","            save_image(image_numpy, img_path)\n","\n","    # errors: same format as |errors| of plotCurrentErrors\n","    def print_current_errors(self, epoch, i, errors, t):\n","        message = '(epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)\n","        for k, v in errors.items():\n","            message += '%s: %.3f ' % (k, v)\n","        print(message)\n","        with open(self.log_name, \"a\") as log_file:\n","            log_file.write('%s\\n' % message)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ssmf_-wEP-L"},"source":["# Networks"]},{"cell_type":"code","metadata":{"id":"fAYOjuf-ERTf"},"source":["class GANLoss(nn.Module):\n","    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n","                 tensor=torch.FloatTensor):\n","        super(GANLoss, self).__init__()\n","        self.real_label = target_real_label\n","        self.fake_label = target_fake_label\n","        self.real_label_var = None\n","        self.fake_label_var = None\n","        self.Tensor = tensor\n","        if use_lsgan:\n","            self.loss = nn.MSELoss()\n","        else:\n","            self.loss = nn.BCELoss()\n","\n","    def get_target_tensor(self, input, target_is_real):\n","        target_tensor = None\n","        if target_is_real:\n","            create_label = ((self.real_label_var is None) or\n","                            (self.real_label_var.numel() != input.numel()))\n","            if create_label:\n","                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n","                self.real_label_var = Variable(real_tensor, requires_grad=False)\n","            target_tensor = self.real_label_var\n","        else:\n","            create_label = ((self.fake_label_var is None) or\n","                            (self.fake_label_var.numel() != input.numel()))\n","            if create_label:\n","                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n","                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n","            target_tensor = self.fake_label_var\n","        return target_tensor\n","\n","    def __call__(self, input, target_is_real):\n","        if isinstance(input[0], list):\n","            loss = 0\n","            for input_i in input:\n","                pred = input_i[-1]\n","                target_tensor = self.get_target_tensor(pred, target_is_real)\n","                loss += self.loss(pred, target_tensor)\n","            return loss\n","        else:\n","            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n","            return self.loss(input[-1], target_tensor)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXv9nTfzGEeZ"},"source":["class ResnetBlock(nn.Module):\n","    def __init__(self, dim, padding_type, norm_layer, activation=nn.ReLU(True), use_dropout=False):\n","        super(ResnetBlock, self).__init__()\n","        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n","\n","    def build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n","        conv_block = []\n","        p = 0\n","        if padding_type == 'reflect':\n","            conv_block += [nn.ReflectionPad2d(1)]\n","        elif padding_type == 'replicate':\n","            conv_block += [nn.ReplicationPad2d(1)]\n","        elif padding_type == 'zero':\n","            p = 1\n","        else:\n","            raise NotImplementedError('padding [%s] is not implemented!' % padding_type)\n","\n","        conv_block += [\n","            nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n","            norm_layer(dim),\n","            activation,\n","        ]\n","\n","        if use_dropout:\n","            conv_block += [nn.Dropout(0.5)]\n","\n","        p = 0\n","        if padding_type == 'reflect':\n","            conv_block += [nn.ReflectionPad2d(1)]\n","        elif padding_type == 'replicate':\n","            conv_block += [nn.ReplicationPad2d(1)]\n","        elif padding_type == 'zero':\n","            p = 1\n","        else:\n","            raise NotImplementedError('padding [%s] is not implemented!' % padding_type)\n","\n","        conv_block += [\n","            nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n","            norm_layer(dim),\n","            activation,\n","        ]\n","\n","        return nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        out = x + self.conv_block(x)\n","        return out\n","\n","class GlobalGenerator(nn.Module):\n","    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9,\n","                 norm_layer=nn.BatchNorm2d, padding_type='reflect'):\n","        assert(n_blocks >= 0)\n","        super(GlobalGenerator, self).__init__()\n","        activation = nn.ReLU(True)\n","\n","        model = [\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n","            norm_layer(ngf),\n","            activation,\n","        ]\n","\n","        # downsample\n","        for i in range(n_downsampling):\n","            mult = 2**i\n","            model += [\n","                nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n","                norm_layer(ngf * mult * 2),\n","                activation,\n","            ]\n","\n","        # resnet blocks\n","        mult = 2**n_downsampling\n","        for i in range(n_blocks):\n","            model += [\n","                ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)\n","            ]\n","\n","        # upsample\n","        for i in range(n_downsampling):\n","            mult = 2**(n_downsampling - i)\n","            model += [\n","                nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n","                norm_layer(int(ngf * mult / 2)),\n","                activation,\n","            ]\n","        model += [\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n","            nn.Tanh(),\n","        ]\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input):\n","        return self.model(input)\n","\n","class MultiscaleDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, num_D=3, use_dropout=False):\n","        super(MultiscaleDiscriminator, self).__init__()\n","        self.num_D = num_D\n","        self.n_layers = n_layers\n","\n","        for i in range(num_D):\n","            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, use_dropout)\n","            setattr(self, 'layer'+str(i), netD.model)\n","\n","        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n","\n","    def singleD_forward(self, model, input):\n","        return [model(input)]\n","\n","    def forward(self, input):\n","        num_D = self.num_D\n","        result = []\n","        input_downsampled = input\n","        for i in range(num_D):\n","            model = getattr(self, 'layer'+str(num_D-1-i))\n","            result.append(self.singleD_forward(model, input_downsampled))\n","            if i != (num_D-1):\n","                input_downsampled = self.downsample(input_downsampled)\n","        return result\n","\n","class NLayerDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, use_dropout=False):\n","        super(NLayerDiscriminator, self).__init__()\n","        self.n_layers = n_layers\n","\n","        kw = 4\n","        padw = int(np.ceil((kw-1.0)/2))\n","        model = [\n","            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        nf = ndf\n","        for n in range(1, n_layers):\n","            nf_prev = nf\n","            nf = min(nf * 2, 512)\n","            model += [\n","                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n","                norm_layer(nf),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        nf_prev = nf\n","        nf = min(nf * 2, 512)\n","        model += [\n","            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n","            norm_layer(nf),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","        model += [nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]\n","\n","        if use_sigmoid:\n","            model += [nn.Sigmoid()]\n","\n","        if use_dropout:\n","            model = model + [nn.Dropout(0.5)]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input):\n","        return self.model(input)\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_nc, output_nc, ngf=64, n_layers=4, norm_layer=nn.BatchNorm2d, ratio=1):\n","        super(Encoder, self).__init__()\n","        self.output_nc = output_nc\n","\n","        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n","                 norm_layer(ngf), nn.ReLU(True)]\n","        ### downsample\n","        for i in range(n_layers):\n","            mult = 2**i\n","            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n","                      norm_layer(ngf * mult * 2), nn.ReLU(True)]\n","\n","        self.downsample = nn.Sequential(*model)\n","        self.pool = nn.AvgPool2d(32)\n","        self.fc = nn.Sequential(*[nn.Linear(int(ngf * mult * 2 * 4 / ratio), 32)])\n","        self.fcVar = nn.Sequential(*[nn.Linear(int(ngf * mult * 2 * 4 / ratio), 32)])\n","\n","        ### upsample\n","        for i in range(n_layers):\n","            mult = 2**(n_layers - i)\n","            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n","                       norm_layer(int(ngf * mult / 2)), nn.ReLU(True)]\n","\n","        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input):\n","        feature = self.model(input)\n","        _conv = self.downsample(input)\n","        _conv = self.pool(_conv)\n","        # print(_conv)\n","        _conv = _conv.view(input.size(0), -1)\n","        #print(_conv.shape)\n","        output = self.fc(_conv)\n","        outputVar = self.fcVar(_conv)\n","        return output, outputVar, feature"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KgjLzXrE3xn"},"source":["def weights_init_gaussian(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('Linear') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm2d') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","def weights_init_uniform(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        init.uniform(m.weight.data, -0.06, 0.06)\n","    elif classname.find('Conv') != -1:\n","        init.uniform(m.weight.data, -0.06, 0.06)\n","    elif classname.find('BatchNorm2d') != -1:\n","        init.uniform(m.weight.data, 0.04, 1.06)\n","        init.constant(m.bias.data, 0.0)\n","\n","def get_norm_layer():\n","    norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n","    return norm_layer\n","\n","def define_G(input_nc, output_nc, ngf, netG, n_downsample_global=3, n_blocks_global=9, norm='instance', ratio=1):\n","    norm_layer = get_norm_layer()\n","    if netG == 'global':\n","        netG = GlobalGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)\n","    elif netG == 'encoder':\n","        netG = Encoder(input_nc, output_nc, 64, n_downsample_global, norm_layer, ratio)\n","    else:\n","        raise NotImplementedError('generator [%s] is not found.' % netG)\n","    netG.apply(weights_init_gaussian)\n","    return netG\n","\n","def define_D(input_nc, ndf, n_layers_D, norm='instance', use_sigmoid=False, num_D=1):\n","    norm_layer = get_norm_layer()\n","    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, use_dropout=False)\n","    netD.apply(weights_init_gaussian)\n","    return netD"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pZCuvQFDVjo"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"UX4CxjXBOJrx"},"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","class CycleGAN():\n","    def name(self):\n","        return 'Bayesian CycleGAN Model'\n","\n","    def initialize(self, opt):\n","        self.opt = opt\n","        self.isTrain = opt.isTrain\n","        if torch.cuda.is_available():\n","            print('cuda is available, we will use gpu!')\n","            self.Tensor = torch.cuda.FloatTensor\n","            #self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","            torch.cuda.manual_seed_all(100)\n","        else:\n","            self.Tensor = torch.FloatTensor\n","            torch.manual_seed(100)\n","        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n","\n","        # get radio for network initialization\n","        ratio = 256 * 256 / opt.loadSize / (opt.loadSize / opt.ratio)\n","\n","        # load network\n","        netG_input_nc = opt.input_nc + 1\n","        netG_output_nc = opt.output_nc + 1\n","        self.netG_A = define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG_A,\n","                                        opt.n_downsample_global, opt.n_blocks_global, opt.norm).type(self.Tensor)#.cuda()\n","        self.netG_B = define_G(netG_output_nc, opt.input_nc, opt.ngf, opt.netG_B,\n","                                        opt.n_downsample_global, opt.n_blocks_global, opt.norm).type(self.Tensor)#.cuda()\n","\n","        self.netE_A = define_G(opt.input_nc, 1, 64, 'encoder', opt.n_downsample_global, norm=opt.norm, ratio=ratio).type(self.Tensor)#.cuda()\n","        self.netE_B = define_G(opt.output_nc, 1, 64, 'encoder', opt.n_downsample_global, norm=opt.norm, ratio=ratio).type(self.Tensor)#.cuda()\n","\n","        if self.isTrain:\n","            use_sigmoid = opt.no_lsgan\n","            self.netD_A = define_D(opt.output_nc, opt.ndf, opt.n_layers_D, opt.norm,\n","                                            use_sigmoid, opt.num_D_A).type(self.Tensor)#.cuda()\n","            self.netD_B = define_D(opt.input_nc, opt.ndf, opt.n_layers_D, opt.norm,\n","                                            use_sigmoid, opt.num_D_B).type(self.Tensor)#.cuda()\n","\n","        if not self.isTrain or opt.continue_train:\n","            self.load_network(self.netG_A, 'G_A', opt.which_epoch, self.save_dir)\n","            self.load_network(self.netG_B, 'G_B', opt.which_epoch, self.save_dir)\n","            self.load_network(self.netE_A, 'E_A', opt.which_epoch, self.save_dir)\n","            self.load_network(self.netE_B, 'E_B', opt.which_epoch, self.save_dir)\n","            if self.isTrain:\n","                self.load_network(self.netD_A, 'D_A', opt.which_epoch, self.save_dir)\n","                self.load_network(self.netD_B, 'D_B', opt.which_epoch, self.save_dir)\n","\n","        # set loss functions and optimizers\n","        if self.isTrain:\n","            self.old_lr = opt.lr\n","            # define loss function\n","            self.criterionGAN = GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n","            self.criterionCycle = torch.nn.L1Loss()\n","            self.criterionL1 = torch.nn.L1Loss()\n","            # initialize optimizers\n","            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_E_A = torch.optim.Adam(self.netE_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_E_B = torch.optim.Adam(self.netE_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n","\n","        print('Network initialized!')\n","\n","        # dataset path and name list\n","        self.origin_path = os.getcwd()\n","        self.path_A = self.opt.dataroot_A\n","        self.path_B = self.opt.dataroot_B\n","        self.list_A = os.listdir(self.path_A)\n","        self.list_B = os.listdir(self.path_B)\n","\n","    def set_input(self, input):\n","        AtoB = self.opt.which_direction == 'AtoB'\n","        self.input_A = input['A' if AtoB else 'B']\n","        self.input_B = input['B' if AtoB else 'A']\n","        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n","\n","    def forward(self):\n","        self.real_A = Variable(self.input_A).type(self.Tensor)#.cuda()\n","        self.real_B = Variable(self.input_B).type(self.Tensor)#.cuda()\n","\n","        # feature map\n","        mc_sample_x = random.sample(self.list_A, self.opt.mc_x)\n","        mc_sample_y = random.sample(self.list_B, self.opt.mc_y)\n","        self.real_B_zx = []\n","        self.real_A_zy = []\n","        self.mu_x = []\n","        self.mu_y = []\n","        self.logvar_x = []\n","        self.logvar_y = []\n","        os.chdir(self.path_A)\n","        for sample_x in mc_sample_x:\n","            z_x = Image.open(sample_x).convert('RGB')\n","            z_x = self.img_resize(z_x, self.opt.loadSize)\n","            z_x = transform(z_x)\n","            if self.opt.input_nc == 1:  # RGB to gray\n","                z_x = z_x[0, ...] * 0.299 + z_x[1, ...] * 0.587 + z_x[2, ...] * 0.114\n","                z_x = z_x.unsqueeze(0)\n","            z_x = Variable(z_x).type(self.Tensor)#.cuda()\n","            z_x = torch.unsqueeze(z_x, 0)\n","            mu_x, logvar_x, feat_map = self.netE_A.forward(z_x)\n","            self.mu_x.append(mu_x)\n","            self.logvar_x.append(logvar_x)\n","            self.feat_map_zx = feat_map\n","            real_B_zx = []\n","            for i in range(0, self.opt.batchSize):\n","                _real = torch.unsqueeze(self.real_B[i], 0)\n","                _real = torch.cat([_real, feat_map], dim=1)\n","                real_B_zx.append(_real)\n","            real_B_zx = torch.cat(real_B_zx)\n","            self.real_B_zx.append(real_B_zx)\n","        self.mu_x = torch.cat(self.mu_x)\n","        self.logvar_x = torch.cat(self.logvar_x)\n","\n","        os.chdir(self.path_B)\n","        for sample_y in mc_sample_y:\n","            z_y = Image.open(sample_y).convert('RGB')\n","            z_y = self.img_resize(z_y, self.opt.loadSize)\n","            z_y = transform(z_y)\n","            if self.opt.output_nc == 1:  # RGB to gray\n","                z_y = z_y[0, ...] * 0.299 + z_y[1, ...] * 0.587 + z_y[2, ...] * 0.114\n","                z_y = z_y.unsqueeze(0)\n","            z_y = Variable(z_y).type(self.Tensor)#.cuda()\n","            z_y = torch.unsqueeze(z_y, 0)\n","\n","            mu_y, logvar_y, feat_map = self.netE_B.forward(z_y)\n","            self.mu_y.append(mu_y)\n","            self.logvar_y.append(logvar_y)\n","            self.feat_map_zy = feat_map\n","            real_A_zy = []\n","            for i in range(0, self.opt.batchSize):\n","                _real = torch.unsqueeze(self.real_A[i], 0)\n","                _real = torch.cat((_real, feat_map), dim=1)\n","                real_A_zy.append(_real)\n","            real_A_zy = torch.cat(real_A_zy)\n","            self.real_A_zy.append(real_A_zy)\n","        self.mu_y = torch.cat(self.mu_y)\n","        self.logvar_y = torch.cat(self.logvar_y)\n","\n","        os.chdir(self.origin_path)\n","\n","    def inference(self):\n","        real_A = Variable(self.input_A).type(self.Tensor)\n","        real_B = Variable(self.input_B).type(self.Tensor)\n","\n","        # feature map\n","        #os.chdir(self.path_A)\n","        #mc_sample_x = random.sample(self.list_A, 1)\n","        #z_x = Image.open(mc_sample_x[0]).convert('RGB')\n","        #z_x = self.img_resize(z_x, self.opt.loadSize)\n","        #z_x = transform(z_x)\n","        #if self.opt.input_nc == 1:  # RGB to gray\n","            #z_x = z_x[0, ...] * 0.299 + z_x[1, ...] * 0.587 + z_x[2, ...] * 0.114\n","            #z_x = z_x.unsqueeze(0)\n","        #if self.opt.use_feat:\n","            #z_x = z_x[0, ...] * 0.299 + z_x[1, ...] * 0.587 + z_x[2, ...] * 0.114\n","            #z_x = z_x.unsqueeze(0)\n","        #z_x = Variable(z_x).type(self.Tensor)\n","        #z_x = torch.unsqueeze(z_x, 0)\n","\n","        #if not self.opt.use_feat:\n","            #mu_x, logvar_x, feat_map_zx = self.netE_A.forward(z_x)\n","        #else:\n","            #feat_map_zx = z_x\n","\n","        os.chdir(self.path_B)\n","        mc_sample_y = random.sample(self.list_B, 1)\n","        z_y = Image.open(mc_sample_y[0]).convert('RGB')\n","        z_y = self.img_resize(z_y, self.opt.loadSize)\n","        z_y = transform(z_y)\n","        if self.opt.output_nc == 1:  # RGB to gray\n","            z_y = z_y[0, ...] * 0.299 + z_y[1, ...] * 0.587 + z_y[2, ...] * 0.114\n","            z_y = z_y.unsqueeze(0)\n","        if self.opt.use_feat:\n","            z_y = z_y[0, ...] * 0.299 + z_y[1, ...] * 0.587 + z_y[2, ...] * 0.114\n","            z_y = z_y.unsqueeze(0)\n","        z_y = Variable(z_y).type(self.Tensor)\n","        z_y = torch.unsqueeze(z_y, 0)\n","\n","        if not self.opt.use_feat:\n","            mu_y, logvar_y, feat_map_zy = self.netE_B.forward(z_y)\n","        else:\n","            feat_map_zy = z_y\n","\n","        os.chdir(self.origin_path)\n","\n","        # combine input image with random feature map\n","        #real_B_zx = []\n","        #for i in range(0, self.opt.batchSize):\n","            #_real = torch.cat((real_B[i:i+1], feat_map_zx), dim=1)\n","            #real_B_zx.append(_real)\n","        #real_B_zx = torch.cat(real_B_zx)\n","        real_A_zy = []\n","        for i in range(0, self.opt.batchSize):\n","            _real = torch.cat((real_A[i:i+1], feat_map_zy), dim=1)\n","            real_A_zy.append(_real)\n","        real_A_zy = torch.cat(real_A_zy)\n","\n","        # inference\n","        fake_B = self.netG_A(real_A_zy)\n","        #fake_B_next = torch.cat((fake_B, feat_map_zx), dim=1)\n","        #self.rec_A = self.netG_B(fake_B_next)\n","        self.fake_B = fake_B\n","\n","        #fake_A = self.netG_B(real_B_zx)\n","        #fake_A_next = torch.cat((fake_A, feat_map_zy), dim=1)\n","        #self.rec_B = self.netG_A(fake_A_next)\n","        #self.fake_A = fake_A\n","\n","    def get_image_paths(self):\n","        return self.image_paths\n","\n","    def img_resize(self, img, target_width):\n","        ow, oh = img.size\n","        if (ow == target_width):\n","            return img\n","        else:\n","            w = target_width\n","            h = int(target_width * oh / ow)\n","        return img.resize((w, h), Image.BICUBIC)\n","\n","    def get_z_random(self, batchSize, nz, random_type='gauss'):\n","        z = self.Tensor(batchSize, nz)\n","        if random_type == 'uni':\n","            z.copy_(torch.rand(batchSize, nz) * 2.0 - 1.0)\n","        elif random_type == 'gauss':\n","            z.copy_(torch.randn(batchSize, nz))\n","        z = Variable(z)\n","        return z\n","\n","    def backward_G(self):\n","        # GAN loss D_A(G_A(A))\n","        fake_B = []\n","        for real_A in self.real_A_zy:\n","            _fake = self.netG_A(real_A)\n","            fake_B.append(_fake)\n","        fake_B = torch.cat(fake_B)\n","\n","        pred_fake = self.netD_A(fake_B)\n","        loss_G_A = self.criterionGAN(pred_fake, True)\n","\n","        # GAN loss D_B(G_B(B))\n","        fake_A = []\n","        for real_B in self.real_B_zx:\n","            _fake = self.netG_B(real_B)\n","            fake_A.append(_fake)\n","        fake_A = torch.cat(fake_A)\n","\n","        pred_fake = self.netD_B(fake_A)\n","        loss_G_B = self.criterionGAN(pred_fake, True)\n","\n","        # cycle loss\n","        lambda_A = self.opt.lambda_A\n","        lambda_B = self.opt.lambda_B\n","\n","        # Forward cycle loss\n","        fake_B_next = []\n","        for i in range(0, fake_B.size(0)):\n","        \t_fake = fake_B[i:(i+1)]\n","        \t_fake = torch.cat((_fake, self.feat_map_zx), dim=1)\n","        \tfake_B_next.append(_fake)\n","        fake_B_next = torch.cat(fake_B_next)\n","\n","        rec_A = self.netG_B(fake_B_next)\n","        loss_cycle_A = 0\n","        for i in range(0, self.opt.mc_y):\n","            loss_cycle_A += self.criterionCycle(rec_A[i*self.real_A.size(0):(i+1)*self.real_A.size(0)], self.real_A) * lambda_A\n","        pred_cycle_G_A = self.netD_B(rec_A)\n","        loss_cycle_G_A = self.criterionGAN(pred_cycle_G_A, True)\n","\n","        # Backward cycle loss\n","        fake_A_next = []\n","        for i in range(0, fake_A.size(0)):\n","        \t_fake = fake_A[i:(i+1)]\n","        \t_fake = torch.cat((_fake, self.feat_map_zy), dim=1)\n","        \tfake_A_next.append(_fake)\n","        fake_A_next = torch.cat(fake_A_next)\n","\n","        rec_B = self.netG_A(fake_A_next)\n","        loss_cycle_B = 0\n","        for i in range(0, self.opt.mc_x):\n","            loss_cycle_B += self.criterionCycle(rec_B[i*self.real_B.size(0):(i+1)*self.real_B.size(0)], self.real_B) * lambda_B\n","        pred_cycle_G_B = self.netD_A(rec_B)\n","        loss_cycle_G_B = self.criterionGAN(pred_cycle_G_B, True)\n","\n","        # prior loss\n","        prior_loss_G_A = self.get_prior(self.netG_A.parameters(), self.opt.batchSize)\n","        prior_loss_G_B = self.get_prior(self.netG_B.parameters(), self.opt.batchSize)\n","\n","        # KL loss\n","        kl_element = self.mu_x.pow(2).add_(self.logvar_x.exp()).mul_(-1).add_(1).add_(self.logvar_x)\n","        loss_kl_EA = torch.sum(kl_element).mul_(-0.5) * self.opt.lambda_kl\n","\n","        kl_element = self.mu_y.pow(2).add_(self.logvar_y.exp()).mul_(-1).add_(1).add_(self.logvar_y)\n","        loss_kl_EB = torch.sum(kl_element).mul_(-0.5) * self.opt.lambda_kl\n","\n","        # total loss\n","        loss_G =  loss_G_A + loss_G_B + (prior_loss_G_A + prior_loss_G_B) + (loss_cycle_G_A + loss_cycle_G_B) * self.opt.gamma + (loss_cycle_A + loss_cycle_B) + (loss_kl_EA + loss_kl_EB)\n","        loss_G.backward()\n","\n","        self.fake_B = fake_B\n","        self.fake_A = fake_A\n","        self.rec_A = rec_A\n","        self.rec_B = rec_B\n","\n","        self.loss_G_A = loss_G_A.item() + loss_cycle_G_A.item() * self.opt.gamma + prior_loss_G_A.item()\n","        self.loss_G_B = loss_G_B.item() + loss_cycle_G_B.item() * self.opt.gamma + prior_loss_G_A.item()\n","        self.loss_cycle_A = loss_cycle_A.item()\n","        self.loss_cycle_B = loss_cycle_B.item()\n","        self.loss_kl_EA = loss_kl_EA.item()\n","        self.loss_kl_EB = loss_kl_EB.item()\n","\n","    def backward_D_A(self):\n","        fake_B = Variable(self.fake_B).type(self.Tensor)#.cuda()\n","        rec_B = Variable(self.rec_B).type(self.Tensor)#.cuda()\n","        # how well it classifiers fake images\n","        pred_fake = self.netD_A(fake_B.detach())\n","        loss_D_fake = self.criterionGAN(pred_fake, False)\n","        pred_cycle_fake = self.netD_A(rec_B.detach())\n","        loss_D_cycle_fake = self.criterionGAN(pred_cycle_fake, False)\n","\n","        # how well it classifiers real images\n","        pred_real = self.netD_A(self.real_B)\n","        loss_D_real = self.criterionGAN(pred_real, True) * self.opt.mc_y\n","\n","        # prior loss\n","        prior_loss_D_A = self.get_prior(self.netD_A.parameters(), self.opt.batchSize)\n","\n","        # total loss\n","        loss_D_A = (loss_D_real + loss_D_fake) * 0.5 + (loss_D_real + loss_D_cycle_fake) * 0.5 * self.opt.gamma + prior_loss_D_A\n","        loss_D_A.backward()\n","        self.loss_D_A = loss_D_A.item()\n","\n","    def backward_D_B(self):\n","        fake_A = Variable(self.fake_A).type(self.Tensor)#.cuda()\n","        rec_A = Variable(self.rec_A).type(self.Tensor)#.cuda()\n","        # how well it classifiers fake images\n","        pred_fake = self.netD_B(fake_A.detach())\n","        loss_D_fake = self.criterionGAN(pred_fake, False)\n","        pred_cycle_fake = self.netD_B(rec_A.detach())\n","        loss_D_cycle_fake = self.criterionGAN(pred_cycle_fake, False)\n","\n","        # how well it classifiers real images\n","        pred_real = self.netD_B(self.real_A)\n","        loss_D_real = self.criterionGAN(pred_real, True) * self.opt.mc_x\n","\n","        # prior loss\n","        prior_loss_D_B = self.get_prior(self.netD_B.parameters(), self.opt.batchSize)\n","\n","        # total loss\n","        loss_D_B = (loss_D_real + loss_D_fake) * 0.5 + (loss_D_real + loss_D_cycle_fake) * 0.5 * self.opt.gamma + prior_loss_D_B\n","        loss_D_B.backward()\n","        self.loss_D_B = loss_D_B.item()\n","\n","\n","    def optimize(self):\n","        # forward\n","        self.forward()\n","        # G_A and G_B\n","        # E_A and E_B\n","        self.optimizer_G.zero_grad()\n","        self.optimizer_E_A.zero_grad()\n","        self.optimizer_E_B.zero_grad()\n","\n","        self.backward_G()\n","        \n","        self.optimizer_G.step()\n","        self.optimizer_E_A.step()\n","        self.optimizer_E_B.step()\n","        # D_A\n","        self.optimizer_D_A.zero_grad()\n","\n","        self.backward_D_A()\n","\n","        self.optimizer_D_A.step()\n","        # D_B\n","        self.optimizer_D_B.zero_grad()\n","\n","        self.backward_D_B()\n","        self.optimizer_D_B.step()\n","\n","    def get_current_loss(self):\n","        loss = OrderedDict([\n","            ('D_A', self.loss_D_A),\n","            ('D_B', self.loss_D_B),\n","            ('G_A', self.loss_G_A),\n","            ('G_B', self.loss_G_B)\n","        ])\n","        if self.opt.gamma == 0:\n","            loss['cyc_A'] = self.loss_cycle_A\n","            loss['cyc_B'] = self.loss_cycle_B\n","        elif self.opt.gamma > 0:\n","            loss['cyc_G_A'] = self.loss_cycle_A\n","            loss['cyc_G_B'] = self.loss_cycle_B\n","        if self.opt.lambda_kl > 0:\n","        \tloss['kl_EA'] = self.loss_kl_EA\n","        \tloss['kl_EB'] = self.loss_kl_EB\n","        return loss\n","\n","    def get_stye_loss(self):\n","        loss = OrderedDict([\n","            ('L1_A', self.loss_G_A_L1),\n","            ('L1_B', self.loss_G_B_L1)\n","        ])\n","        return loss\n","\n","    def get_current_visuals(self):\n","        #real_A = tensor2im(self.input_A)\n","        fake_B = tensor2im(self.fake_B)\n","        #rec_A = tensor2im(self.rec_A)\n","        #real_B = tensor2im(self.input_B)\n","        #fake_A = tensor2im(self.fake_A)\n","        #rec_B = tensor2im(self.rec_B)\n","        #visuals = OrderedDict([\n","            #('real_A', real_A),\n","            #('fake_B', fake_B),\n","            #('rec_A', rec_A),\n","            #('real_B', real_B),\n","            #('fake_A', fake_A),\n","            #('rec_B', rec_B)\n","        #])\n","        visuals = OrderedDict([('fake_B', fake_B)])\n","        return visuals\n","\n","    def get_prior(self, parameters, dataset_size):\n","        prior_loss = Variable(torch.zeros((1))).cuda()\n","        for param in parameters:\n","            prior_loss += torch.mean(param*param)\n","        return prior_loss / dataset_size\n","\n","    def save_model(self, label):\n","        self.save_network(self.netG_A, 'G_A', label)\n","        self.save_network(self.netG_B, 'G_B', label)\n","        self.save_network(self.netE_A, 'E_A', label)\n","        self.save_network(self.netE_B, 'E_B', label)\n","        self.save_network(self.netD_A, 'D_A', label)\n","        self.save_network(self.netD_B, 'D_B', label)\n","\n","    def load_network(self, network, network_label, epoch_label, save_dir=''):\n","        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n","        save_path = os.path.join(self.save_dir, save_filename)\n","        try:\n","            network.load_state_dict(torch.load(save_path))\n","        except:\n","            pretrained_dict = torch.load(save_path)\n","            model_dict = network.state_dict()\n","            try:\n","                pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n","                network.load_state_dict(pretrained_dict)\n","                print('Pretrained network %s has excessive layers; Only loading layers that are used' % network_label)\n","            except:\n","                print('Pretrained network %s has fewer layers; The following are not initialized:' % network_label)\n","                if sys.version_info >= (3, 0):\n","                    not_initialized = set()\n","                else:\n","                    from sets import Set\n","                    not_initialized = Set()\n","                for k, v in pretrained_dict.items():\n","                    if v.size() == model_dict[k].size():\n","                        model_dict[k] = v\n","\n","                for k, v in model_dict.items():\n","                    if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n","                        not_initialized.add(k.split('.')[0])\n","                print(sorted(not_initialized))\n","                network.load_state_dict(model_dict)\n","\n","    def save_network(self, network, network_label, epoch_label):\n","        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n","        save_path = os.path.join(self.save_dir, save_filename)\n","        torch.save(network.cpu().state_dict(), save_path)\n","        if torch.cuda.is_available():\n","            network.cuda()\n","\n","    def print_network(self, net):\n","        num_params = 0\n","        for param in net.parameters():\n","            num_params += param.numel()\n","        print(net)\n","        print('Total number of parameters: %d' % num_params)\n","\n","    # update learning rate (called once every iter)\n","    def update_learning_rate(self, epoch, epoch_iter, dataset_size):\n","        # lrd = self.opt.lr / self.opt.niter_decay\n","        if epoch > self.opt.niter:\n","            lr = self.opt.lr * np.exp(-1.0 * min(1.0, epoch_iter/float(dataset_size)))\n","            for param_group in self.optimizer_D_A.param_groups:\n","                param_group['lr'] = lr\n","            for param_group in self.optimizer_D_B.param_groups:\n","                param_group['lr'] = lr\n","            for param_group in self.optimizer_G.param_groups:\n","                param_group['lr'] = lr\n","            print('update learning rate: %f -> %f' % (self.old_lr, lr))\n","            self.old_lr = lr\n","        else:\n","            lr = self.old_lr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7KroGBaK9r5"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"DODWhJp1QBpe"},"source":["IMG_EXTENSIONS = [\n","    '.jpg', '.JPG', '.jpeg', '.JPEG',\n","    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n","]\n","\n","class BaseDataLoader():\n","    def __init__(self):\n","        pass\n","    \n","    def initialize(self, opt):\n","        self.opt = opt\n","        pass\n","\n","    def load_data():\n","        return None\n","\n","class BaseDataset(data.Dataset):\n","    def __init__(self):\n","        super(BaseDataset, self).__init__()\n","\n","    def name(self):\n","        return 'BaseDataset'\n","\n","    def initialize(self, opt):\n","        pass\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n","\n","\n","def make_dataset(dir):\n","    images = []\n","    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n","\n","    for root, _, fnames in sorted(os.walk(dir)):\n","        for fname in fnames:\n","            if is_image_file(fname):\n","                path = os.path.join(root, fname)\n","                images.append(path)\n","\n","    return images\n","\n","def get_transform(opt):\n","    transform_list = []\n","    if opt.resize_or_crop == 'resize':         # 1024 x 1024\n","        osize = [opt.loadSize, opt.loadSize]\n","        transform_list.append(transforms.Scale(osize, Image.BICUBIC))\n","        # transform_list.append(transforms.RandomCrop(opt.fineSize))\n","    elif opt.resize_or_crop == 'crop':\n","        transform_list.append(transforms.RandomCrop(opt.fineSize))\n","    elif opt.resize_or_crop == 'scale_width':  # 1024 x 512\n","        transform_list.append(transforms.Lambda(\n","            lambda img: __scale_width(img, opt.loadSize)))\n","    elif opt.resize_or_crop == 'scale_width_and_crop':\n","        transform_list.append(transforms.Lambda(\n","            lambda img: __scale_width(img, opt.loadSize)))\n","        transform_list.append(transforms.RandomCrop(opt.fineSize))\n","\n","    if opt.isTrain and not opt.no_flip:\n","        transform_list.append(transforms.RandomHorizontalFlip())\n","\n","    transform_list += [transforms.ToTensor(),\n","                       transforms.Normalize((0.5, 0.5, 0.5),\n","                                            (0.5, 0.5, 0.5))]\n","    return transforms.Compose(transform_list)\n","\n","def __scale_width(img, target_width):\n","    ow, oh = img.size\n","    if (ow == target_width):\n","        return img\n","    else:\n","        w = target_width\n","        h = int(target_width * oh / ow)\n","    return img.resize((w, h), Image.BICUBIC)\n","\n","class UnalignedDataset(BaseDataset):\n","    def initialize(self, opt):\n","        self.opt = opt\n","        self.dir_A = opt.dataroot_A\n","        self.dir_B = opt.dataroot_B\n","\n","        self.A_paths = make_dataset(self.dir_A)\n","        self.B_paths = make_dataset(self.dir_B)\n","\n","        self.A_paths = sorted(self.A_paths)\n","        self.B_paths = sorted(self.B_paths)\n","        self.A_size = len(self.A_paths)\n","        self.B_size = len(self.B_paths)\n","        self.transform = get_transform(opt)\n","\n","    def __getitem__(self, index):\n","        A_path = self.A_paths[index % self.A_size]\n","        index_A = index % self.A_size\n","        if self.opt.serial_batches:\n","            index_B = index % self.B_size\n","        else:\n","            index_B = random.randint(0, self.B_size - 1)\n","        B_path = self.B_paths[index_B]\n","        # print('(A, B) = (%d, %d)' % (index_A, index_B))\n","        A_img = Image.open(A_path).convert('RGB')\n","        B_img = Image.open(B_path).convert('RGB')\n","\n","        A = self.transform(A_img)\n","        B = self.transform(B_img)\n","        if self.opt.which_direction == 'BtoA':\n","            input_nc = self.opt.output_nc\n","            output_nc = self.opt.input_nc\n","        else:\n","            input_nc = self.opt.input_nc\n","            output_nc = self.opt.output_nc\n","\n","        if input_nc == 1:  # RGB to gray\n","            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n","            A = tmp.unsqueeze(0)\n","\n","        if output_nc == 1:  # RGB to gray\n","            tmp = B[0, ...] * 0.299 + B[1, ...] * 0.587 + B[2, ...] * 0.114\n","            B = tmp.unsqueeze(0)\n","        return {'A': A, 'B': B,\n","                'A_paths': A_path, 'B_paths': B_path}\n","\n","    def __len__(self):\n","        return max(self.A_size, self.B_size)\n","\n","    def name(self):\n","        return 'UnalignedDataset'\n","\n","def CreateDataset(opt):\n","    dataset = None\n","    dataset = UnalignedDataset()\n","    print(\"dataset [%s] was created\" % (dataset.name()))\n","    dataset.initialize(opt)\n","    # dataset.__getitem__(1)\n","    return dataset\n","\n","class CustomDatasetDataLoader(BaseDataLoader):\n","    def name(self):\n","        return 'CustomDatasetDataLoader'\n","\n","    def initialize(self, opt):\n","        BaseDataLoader.initialize(self, opt)\n","        self.dataset = CreateDataset(opt)\n","        self.dataloader = torch.utils.data.DataLoader(\n","            self.dataset,\n","            batch_size=opt.batchSize,\n","            shuffle=not opt.serial_batches,\n","            num_workers=int(opt.nThreads))\n","\n","    def load_data(self):\n","        return self\n","\n","    def __len__(self):\n","        return min(len(self.dataset), self.opt.max_dataset_size)\n","\n","    def __iter__(self):\n","        for i, data in enumerate(self.dataloader):\n","            if i >= self.opt.max_dataset_size:\n","                break\n","            yield data\n","\n","def CreateDataLoader(opt):\n","    data_loader = CustomDatasetDataLoader()\n","    data_loader.initialize(opt)\n","    return data_loader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzqVSk4qNlic"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"0YIGMHLxXx4p"},"source":["class Config:\n","  def __init__(self):\n","    self.name = \"Monet\"\n","    self.checkpoints_dir = \"/content/drive/MyDrive/photo2monet/cycleganbayesian/\"\n","    self.model = 'CycleGAN'\n","    self.norm = 'instance'\n","    self.use_dropout = False\n","    self.gpu_ids = '0'\n","    self.which_direction = 'AtoB'\n","\n","    self.batchSize = 1\n","    self.loadSize = 256\n","    self.ratio = 1\n","    self.fineSize = 256\n","    self.input_nc = 3\n","    self.output_nc = 3\n","\n","    self.dataroot_A = '/content/photo_jpg/'\n","    self.dataroot_B = '/content/drive/MyDrive/photo2monet/monetphotos/'\n","    self.resize_or_crop = \"scale_width\"\n","    self.serial_batches = False\n","    self.no_flip = True\n","    self.nThreads = 1\n","    self.max_dataset_size = float(\"inf\")\n","\n","    self.display_winsize = 256\n","    self.display_id = 0\n","    self.display_port = 8097\n","\n","    self.netG_A = 'global'\n","    self.netG_B = 'global'\n","    self.ngf = 32\n","    self.n_downsample_global = 2\n","    self.n_blocks_global = 6\n","\n","    self.netD = 'mult_sacle'\n","    self.num_D_A = 1\n","    self.num_D_B = 1\n","    self.n_layers_D = 3\n","    self.ndf = 64\n","\n","    self.initialized = True\n","    self.isTrain = False\n","\n","    self.display_freq = 100\n","    self.display_single_pane_ncols = 0\n","    self.update_html_freq = 1000\n","    self.print_freq = 100\n","    self.save_latest_freq = 5000\n","    self.save_epoch_freq = 5\n","\n","    self.continue_train = True\n","    self.gamma = 0.1\n","    self.epoch_count = 1\n","    self.phase = \"train\"\n","    self.which_epoch = \"latest\"\n","    self.niter = 50\n","    self.niter_decay = 50\n","    self.beta1 = 0.5\n","    self.lr = 0.0002\n","    self.no_lsgan = False\n","    self.lambda_A = 10.0\n","    self.lambda_B = 10.0\n","    self.lambda_kl = 0.1\n","    self.mc_y = 3\n","    self.mc_x = 3\n","    self.no_html = False\n","    self.lr_policy = 'lambda'\n","    self.lr_decay_iters = 50\n","    self.debug = False\n","    self.need_match = False\n","    self.use_feat = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Dikqx1vYuiG","executionInfo":{"status":"ok","timestamp":1611520575261,"user_tz":-60,"elapsed":15627,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"786e086e-5ee2-4c40-cf53-768a9bc3661d"},"source":["opt = Config()\n","data_loader = CreateDataLoader(opt)\n","dataset = data_loader.load_data()\n","dataset_size = len(data_loader)\n","print('training images = %d' % dataset_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dataset [UnalignedDataset] was created\n","training images = 7038\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kQsdd3pkmrMq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611080301663,"user_tz":-60,"elapsed":4540,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"58225c6a-e37f-48b5-d482-1c810dc1b3cc"},"source":["# continue train or not\n","if opt.continue_train:\n","    start_epoch = 11\n","    epoch_iter = 0\n","    print('Resuming from epoch %d at iteration %d' % (start_epoch, epoch_iter))\n","else:\n","    start_epoch, epoch_iter = 1, 0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Resuming from epoch 11 at iteration 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IOHHlShmCWsI"},"source":["visualizer = Visualizer(opt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qh7CtkXIedvS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611520606252,"user_tz":-60,"elapsed":16123,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"f6854a7f-c072-4d28-973f-cf8cdbbd1fdb"},"source":["model = CycleGAN()\n","model.initialize(opt)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda is available, we will use gpu!\n","Network initialized!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOyuUGjRma9z","executionInfo":{"status":"ok","timestamp":1611521088283,"user_tz":-60,"elapsed":479170,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"3e8daa13-369b-4c34-98c4-b67c07c06372"},"source":["img_dir = \"/content/drive/MyDrive/photo2monet/cycleganbayesian/epoch40/\"\n","for i, data in enumerate(dataset):\n","  model.set_input(data)\n","  model.inference()\n","  visuals = model.get_current_visuals()\n","  for label, image_numpy in visuals.items():\n","    img_path = os.path.join(img_dir, f'{i}.jpg')\n","    save_image(image_numpy, img_path)\n","  if i%100 == 0:\n","    print(f'{i} images saved')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 images saved\n","100 images saved\n","200 images saved\n","300 images saved\n","400 images saved\n","500 images saved\n","600 images saved\n","700 images saved\n","800 images saved\n","900 images saved\n","1000 images saved\n","1100 images saved\n","1200 images saved\n","1300 images saved\n","1400 images saved\n","1500 images saved\n","1600 images saved\n","1700 images saved\n","1800 images saved\n","1900 images saved\n","2000 images saved\n","2100 images saved\n","2200 images saved\n","2300 images saved\n","2400 images saved\n","2500 images saved\n","2600 images saved\n","2700 images saved\n","2800 images saved\n","2900 images saved\n","3000 images saved\n","3100 images saved\n","3200 images saved\n","3300 images saved\n","3400 images saved\n","3500 images saved\n","3600 images saved\n","3700 images saved\n","3800 images saved\n","3900 images saved\n","4000 images saved\n","4100 images saved\n","4200 images saved\n","4300 images saved\n","4400 images saved\n","4500 images saved\n","4600 images saved\n","4700 images saved\n","4800 images saved\n","4900 images saved\n","5000 images saved\n","5100 images saved\n","5200 images saved\n","5300 images saved\n","5400 images saved\n","5500 images saved\n","5600 images saved\n","5700 images saved\n","5800 images saved\n","5900 images saved\n","6000 images saved\n","6100 images saved\n","6200 images saved\n","6300 images saved\n","6400 images saved\n","6500 images saved\n","6600 images saved\n","6700 images saved\n","6800 images saved\n","6900 images saved\n","7000 images saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VvbClWcyqXSP"},"source":[""],"execution_count":null,"outputs":[]}]}